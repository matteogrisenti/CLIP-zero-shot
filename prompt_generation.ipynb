{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60897f0",
   "metadata": {},
   "source": [
    "# Prompt generation\n",
    "\n",
    "This is a temporary notebook in which we will generate the prompt for clip given the set of classes from Flowers102.\n",
    "\n",
    "Specifically, we first generate the prompt from scratch using a LLDM for each class, then we will refine a specific set of prompts given the classes and the previously generated prompts. This latter step will be employed in case CLIP will predict different class with similar probability in order to create a more detailed prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e532365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "234bc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3730176",
   "metadata": {},
   "source": [
    "**Loading the classes from Flowers102**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c28aa8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4afaa584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_novel_categories(dataset):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efb92d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Class Names: [(0, 'pink primrose'), (1, 'hard-leaved pocket orchid'), (2, 'canterbury bells'), (3, 'sweet pea'), (4, 'english marigold'), (5, 'tiger lily'), (6, 'moon orchid'), (7, 'bird of paradise'), (8, 'monkshood'), (9, 'globe thistle'), (10, 'snapdragon'), (11, \"colt's foot\"), (12, 'king protea'), (13, 'spear thistle'), (14, 'yellow iris'), (15, 'globe-flower'), (16, 'purple coneflower'), (17, 'peruvian lily'), (18, 'balloon flower'), (19, 'giant white arum lily'), (20, 'fire lily'), (21, 'pincushion flower'), (22, 'fritillary'), (23, 'red ginger'), (24, 'grape hyacinth'), (25, 'corn poppy'), (26, 'prince of wales feathers'), (27, 'stemless gentian'), (28, 'artichoke'), (29, 'sweet william'), (30, 'carnation'), (31, 'garden phlox'), (32, 'love in the mist'), (33, 'mexican aster'), (34, 'alpine sea holly'), (35, 'ruby-lipped cattleya'), (36, 'cape flower'), (37, 'great masterwort'), (38, 'siam tulip'), (39, 'lenten rose'), (40, 'barbeton daisy'), (41, 'daffodil'), (42, 'sword lily'), (43, 'poinsettia'), (44, 'bolero deep blue'), (45, 'wallflower'), (46, 'marigold'), (47, 'buttercup'), (48, 'oxeye daisy'), (49, 'common dandelion'), (50, 'petunia')]\n",
      "Novel Class Names: [(51, 'wild pansy'), (52, 'primula'), (53, 'sunflower'), (54, 'pelargonium'), (55, 'bishop of llandaff'), (56, 'gaura'), (57, 'geranium'), (58, 'orange dahlia'), (59, 'pink-yellow dahlia?'), (60, 'cautleya spicata'), (61, 'japanese anemone'), (62, 'black-eyed susan'), (63, 'silverbush'), (64, 'californian poppy'), (65, 'osteospermum'), (66, 'spring crocus'), (67, 'bearded iris'), (68, 'windflower'), (69, 'tree poppy'), (70, 'gazania'), (71, 'azalea'), (72, 'water lily'), (73, 'rose'), (74, 'thorn apple'), (75, 'morning glory'), (76, 'passion flower'), (77, 'lotus'), (78, 'toad lily'), (79, 'anthurium'), (80, 'frangipani'), (81, 'clematis'), (82, 'hibiscus'), (83, 'columbine'), (84, 'desert-rose'), (85, 'tree mallow'), (86, 'magnolia'), (87, 'cyclamen'), (88, 'watercress'), (89, 'canna lily'), (90, 'hippeastrum'), (91, 'bee balm'), (92, 'ball moss'), (93, 'foxglove'), (94, 'bougainvillea'), (95, 'camellia'), (96, 'mallow'), (97, 'mexican petunia'), (98, 'bromelia'), (99, 'blanket flower'), (100, 'trumpet creeper'), (101, 'blackberry lily')]\n"
     ]
    }
   ],
   "source": [
    "_, _, tmp_test = get_data()\n",
    "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
    "print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64bb4ae",
   "metadata": {},
   "source": [
    "I will simply use the variable `CLASS_NAMES` as list of classes.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4800fbe0",
   "metadata": {},
   "source": [
    "# Loadig LLM (LLaMA 3.1 8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5de63f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama if not already installed\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh\n",
    "# !pip install ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007086a",
   "metadata": {},
   "source": [
    "## Prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d57bc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "def generate_prompts(classes=[], model_name=\"llama3.2-clip-prompts\"):\n",
    "    \"\"\"\n",
    "    Generate prompts for a list of classes using a custom LLaMa model.\n",
    "    Args:\n",
    "        model: The LLaDA model to use for generating prompts.\n",
    "        classes: A list of class names for which to generate prompts.\n",
    "    Returns:\n",
    "        A dictionary where keys are class names and values are generated prompts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unifying all the classes into a single string, that will be the input prompt together with the system prompt\n",
    "    class_string = \", \".join(classes)\n",
    "    \n",
    "    response: ChatResponse = chat(model=model_name, messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': class_string,\n",
    "        },\n",
    "    ])\n",
    "    \n",
    "    captions = response.message.content\n",
    "    \n",
    "    # Prompts are separated by semicolons, so we split them to create a list\n",
    "    extracted_prompts = captions.split(';')\n",
    "    extracted_prompts = [prompt.strip() for prompt in extracted_prompts if prompt.strip()]\n",
    "    \n",
    "    # Building dictionary (class name -> prompt)\n",
    "    class_prompt_dict = dict(zip(classes, extracted_prompts))\n",
    "    \n",
    "    return class_prompt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94bca1",
   "metadata": {},
   "source": [
    "### ðŸ§ª Mini test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c59489a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pink primrose': 'Soft pink petals with delicate edges',\n",
       " 'hard-leaved pocket orchid': 'waxy, leathery leaves with a prominent midrib',\n",
       " 'canterbury bells': 'drooping, bell-shaped flowers in shades of blue and white',\n",
       " 'sweet pea': 'pastel-colored, fragrant blooms with intricate patterns on the petals',\n",
       " 'english marigold': 'bright orange-yellow petals with a prominent center.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts = generate_prompts(CLASS_NAMES[:5])\n",
    "test_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259057b",
   "metadata": {},
   "source": [
    "# Testing class generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6ed113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n",
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe84b5",
   "metadata": {},
   "source": [
    "### Testing with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37c3f000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pink primrose': 'Here are the captions for each flower class:\\n\\nA close-up photo of a Pink Primrose, a delicate, cup-shaped flower with pale pink petals and a prominent center',\n",
       " 'hard-leaved pocket orchid': 'A photograph of a Hard-leaved Pocket Orchid, an epiphytic orchid with long, thin leaves and small white flowers',\n",
       " 'canterbury bells': 'An image of a Canterbury Bell, a bell-shaped flower with bright blue petals and a distinctive shape',\n",
       " 'sweet pea': 'A photo of a Sweet Pea, a fragrant, pastel-colored flower with delicate, ruffled petals',\n",
       " 'english marigold': 'A photograph of an English Marigold, a bright yellow flower with daisy-like petals and a strong, pungent scent',\n",
       " 'tiger lily': 'A close-up of a Tiger Lily, a large, showy flower with bright orange petals and long green stems',\n",
       " 'moon orchid': 'An image of a Moon Orchid, a white or pale-colored flower with delicate, curved petals',\n",
       " 'bird of paradise': 'A photograph of a Bird of Paradise, a striking, exotic flower with bright orange and purple markings',\n",
       " 'monkshood': 'A close-up of a Monkshood, a blue or white flower with distinctive, spiky petals',\n",
       " 'globe thistle': 'A photo of a Globe Thistle, a prickly, globe-shaped flower with purple or pink petals',\n",
       " 'snapdragon': 'A photograph of a Snapdragon, a tall, colorful flower with long, thin petals',\n",
       " \"colt's foot\": \"An image of a Colt's Foot, a small, yellowish-green flower with delicate, lacy petals\",\n",
       " 'king protea': 'A close-up of a King Protea, a large, showy flower with bright yellow and orange markings',\n",
       " 'spear thistle': 'A photo of a Spear Thistle, a prickly, spiky flower with purple or pink petals',\n",
       " 'yellow iris': 'An image of a Yellow Iris, a cup-shaped flower with bright yellow petals and long green stems',\n",
       " 'globe-flower': 'A photograph of a Globe-Flower, a delicate, globe-shaped flower with white or pale-colored petals',\n",
       " 'purple coneflower': 'A close-up of a Purple Coneflower, a large, showy flower with bright purple petals and a prominent center',\n",
       " 'peruvian lily': 'An image of a Peruvian Lily, a tall, elegant flower with white or pale-colored petals',\n",
       " 'balloon flower': 'A photo of a Balloon Flower, a delicate, balloon-shaped flower with white or pale-colored petals',\n",
       " 'giant white arum lily': 'A close-up of a Giant White Arum Lily, a large, showy flower with bright white petals and long green stems',\n",
       " 'fire lily': 'A photograph of a Fire Lily, a striking, orange-red flower with long, thin petals',\n",
       " 'pincushion flower': 'An image of a Pincushion Flower, a small, daisy-like flower with bright yellow or pink petals',\n",
       " 'fritillary': 'A close-up of a Fritillary, a delicate, cup-shaped flower with bright colors and distinctive markings',\n",
       " 'red ginger': 'A photo of Red Ginger, a small, orange-red flower with long, thin petals',\n",
       " 'grape hyacinth': 'An image of a Grape Hyacinth, a small, bell-shaped flower with blue or purple petals',\n",
       " 'corn poppy': 'A photograph of a Corn Poppy, a delicate, red flower with four petals',\n",
       " 'prince of wales feathers': 'A close-up of the Prince of Wales Feathers, a tall, colorful flower with bright red and yellow markings',\n",
       " 'stemless gentian': 'A photo of a Stemless Gentian, a delicate, blue flower with distinctive, curved petals',\n",
       " 'artichoke': 'An image of an Artichoke, a large, showy flower with bright purple or pink petals',\n",
       " 'sweet william': 'A close-up of Sweet William, a small, fragrant flower with delicate, ruffled petals',\n",
       " 'carnation': 'A photograph of a Carnation, a long-lasting, cup-shaped flower with bright colors and distinctive markings',\n",
       " 'garden phlox': 'A photo of Garden Phlox, a fragrant, pinkish-purple flower with delicate, ruffled petals',\n",
       " 'love in the mist': 'An image of Love in the Mist, a small, white or pale-colored flower with delicate, lacy petals',\n",
       " 'mexican aster': 'A photograph of Mexican Aster, a small, daisy-like flower with bright blue or purple petals',\n",
       " 'alpine sea holly': 'A close-up of Alpine Sea Holly, a prickly, spiky flower with white or pale-colored petals',\n",
       " 'ruby-lipped cattleya': 'A photo of Ruby-Lipped Cattleya, an epiphytic orchid with long, thin leaves and bright red flowers',\n",
       " 'cape flower': 'An image of the Cape Flower, a large, showy flower with bright yellow petals and distinctive markings',\n",
       " 'great masterwort': 'A close-up of Great Masterwort, a tall, colorful flower with bright blue or purple markings',\n",
       " 'siam tulip': 'A photo of the Siam Tulip, a delicate, cup-shaped flower with bright orange or red petals',\n",
       " 'lenten rose': 'An image of Lenten Rose, a small, fragrant flower with delicate, ruffled petals.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! Generate prompts for all classes\n",
    "flower_prompts_dict = generate_prompts(CLASS_NAMES[:40]) # it should take 1 minute or less\n",
    "flower_prompts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8939c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets select just sample from classes of the generated prompts\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "my_category_samples = []\n",
    "for sample_id, label in enumerate(train_set._labels):\n",
    "    if CLASS_NAMES[label] in flower_prompts_dict.keys():\n",
    "        my_category_samples.append(sample_id)\n",
    "        \n",
    "my_train_set = torch.utils.data.Subset(train_set, my_category_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ed27576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install --yes -c pytorch pytorch=2.7.1 torchvision cudatoolkit=12.8\n",
    "# !pip install ftfy regex tqdm\n",
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bed614f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7fbae6cf4280>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# preprocess contains CLIP's pre-defined augmentations, let's inspect them!\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30224e3f",
   "metadata": {},
   "source": [
    "### Classic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a05684bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Base classes accuracy: 62.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # we don't want gradients\n",
    "def eval(clip_model, dataset, categories, batch_size, device, label=\"\"):\n",
    "    # let's set the model in evaluation mode\n",
    "    clip_model.eval()\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    \n",
    "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
    "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
    "    text_inputs = clip.tokenize(\n",
    "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
    "    ).to(device)\n",
    "\n",
    "    # we can encode the text features once as they are shared for all images\n",
    "    # therefore we do it outside the evaluation loop\n",
    "    text_features = clip_model.encode_text(text_inputs)\n",
    "    # and here we normalize them (standard pratice with CLIP)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward image through CLIP image encoder\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item() \n",
    "        \n",
    "        # for i in range(len(predicted_class != target)):\n",
    "        #     if not predicted_class[i] == target[i]:\n",
    "        #         print(f\"Image {i} predicted as {CLASS_NAMES[predicted_class[i]]}, but it is {CLASS_NAMES[target[i]]}\")\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = eval(clip_model=clip_model, dataset=my_train_set, categories=[i for i in range(len(CLASS_NAMES))], batch_size=128, device=device, label=\"ðŸ§  Zero-shot evaluation on Base Classes\")\n",
    "\n",
    "print()\n",
    "print(f\"ðŸ” Base classes accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed925176",
   "metadata": {},
   "source": [
    "### Out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a59dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Base classes accuracy: 65.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # we don't want gradients\n",
    "def eval(clip_model, dataset, prompt_dict, batch_size, device, label=\"\"):\n",
    "    # let's set the model in evaluation mode\n",
    "    clip_model.eval()\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {idx: idx for idx, cat in enumerate(prompt_dict.keys())}\n",
    "    \n",
    "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
    "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
    "    text_inputs = clip.tokenize(\n",
    "        [prompt for prompt in prompt_dict.values()]\n",
    "    ).to(device)\n",
    "\n",
    "    # we can encode the text features once as they are shared for all images\n",
    "    # therefore we do it outside the evaluation loop\n",
    "    text_features = clip_model.encode_text(text_inputs)\n",
    "    # and here we normalize them (standard pratice with CLIP)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward image through CLIP image encoder\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "        \n",
    "        # if (predicted_class != target).any():\n",
    "        #     # if there are any wrong predictions, print them\n",
    "        #     for i in range(len(predicted_class)):\n",
    "        #         if predicted_class[i] != target[i]:\n",
    "        #             print(f\"Image: {i}, Wrong prediction: {CLASS_NAMES[predicted_class[i]]} (predicted) vs {CLASS_NAMES[target[i]]} (target)\")\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = eval(clip_model=clip_model, dataset=my_train_set, prompt_dict=flower_prompts_dict, batch_size=128, device=device, label=\"ðŸ§  Zero-shot evaluation on Base Classes\")\n",
    "\n",
    "print()\n",
    "print(f\"ðŸ” Base classes accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5776169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompts:\n",
      "========================================\n",
      "\"pink primrose\": \"Here are the captions for each flower class:\n",
      "\n",
      "A close-up photo of a Pink Primrose, a delicate, cup-shaped flower with pale pink petals and a prominent center\"\n",
      "\"hard-leaved pocket orchid\": \"A photograph of a Hard-leaved Pocket Orchid, an epiphytic orchid with long, thin leaves and small white flowers\"\n",
      "\"canterbury bells\": \"An image of a Canterbury Bell, a bell-shaped flower with bright blue petals and a distinctive shape\"\n",
      "\"sweet pea\": \"A photo of a Sweet Pea, a fragrant, pastel-colored flower with delicate, ruffled petals\"\n",
      "\"english marigold\": \"A photograph of an English Marigold, a bright yellow flower with daisy-like petals and a strong, pungent scent\"\n",
      "\"tiger lily\": \"A close-up of a Tiger Lily, a large, showy flower with bright orange petals and long green stems\"\n",
      "\"moon orchid\": \"An image of a Moon Orchid, a white or pale-colored flower with delicate, curved petals\"\n",
      "\"bird of paradise\": \"A photograph of a Bird of Paradise, a striking, exotic flower with bright orange and purple markings\"\n",
      "\"monkshood\": \"A close-up of a Monkshood, a blue or white flower with distinctive, spiky petals\"\n",
      "\"globe thistle\": \"A photo of a Globe Thistle, a prickly, globe-shaped flower with purple or pink petals\"\n",
      "\"snapdragon\": \"A photograph of a Snapdragon, a tall, colorful flower with long, thin petals\"\n",
      "\"colt's foot\": \"An image of a Colt's Foot, a small, yellowish-green flower with delicate, lacy petals\"\n",
      "\"king protea\": \"A close-up of a King Protea, a large, showy flower with bright yellow and orange markings\"\n",
      "\"spear thistle\": \"A photo of a Spear Thistle, a prickly, spiky flower with purple or pink petals\"\n",
      "\"yellow iris\": \"An image of a Yellow Iris, a cup-shaped flower with bright yellow petals and long green stems\"\n",
      "\"globe-flower\": \"A photograph of a Globe-Flower, a delicate, globe-shaped flower with white or pale-colored petals\"\n",
      "\"purple coneflower\": \"A close-up of a Purple Coneflower, a large, showy flower with bright purple petals and a prominent center\"\n",
      "\"peruvian lily\": \"An image of a Peruvian Lily, a tall, elegant flower with white or pale-colored petals\"\n",
      "\"balloon flower\": \"A photo of a Balloon Flower, a delicate, balloon-shaped flower with white or pale-colored petals\"\n",
      "\"giant white arum lily\": \"A close-up of a Giant White Arum Lily, a large, showy flower with bright white petals and long green stems\"\n",
      "\"fire lily\": \"A photograph of a Fire Lily, a striking, orange-red flower with long, thin petals\"\n",
      "\"pincushion flower\": \"An image of a Pincushion Flower, a small, daisy-like flower with bright yellow or pink petals\"\n",
      "\"fritillary\": \"A close-up of a Fritillary, a delicate, cup-shaped flower with bright colors and distinctive markings\"\n",
      "\"red ginger\": \"A photo of Red Ginger, a small, orange-red flower with long, thin petals\"\n",
      "\"grape hyacinth\": \"An image of a Grape Hyacinth, a small, bell-shaped flower with blue or purple petals\"\n",
      "\"corn poppy\": \"A photograph of a Corn Poppy, a delicate, red flower with four petals\"\n",
      "\"prince of wales feathers\": \"A close-up of the Prince of Wales Feathers, a tall, colorful flower with bright red and yellow markings\"\n",
      "\"stemless gentian\": \"A photo of a Stemless Gentian, a delicate, blue flower with distinctive, curved petals\"\n",
      "\"artichoke\": \"An image of an Artichoke, a large, showy flower with bright purple or pink petals\"\n",
      "\"sweet william\": \"A close-up of Sweet William, a small, fragrant flower with delicate, ruffled petals\"\n",
      "\"carnation\": \"A photograph of a Carnation, a long-lasting, cup-shaped flower with bright colors and distinctive markings\"\n",
      "\"garden phlox\": \"A photo of Garden Phlox, a fragrant, pinkish-purple flower with delicate, ruffled petals\"\n",
      "\"love in the mist\": \"An image of Love in the Mist, a small, white or pale-colored flower with delicate, lacy petals\"\n",
      "\"mexican aster\": \"A photograph of Mexican Aster, a small, daisy-like flower with bright blue or purple petals\"\n",
      "\"alpine sea holly\": \"A close-up of Alpine Sea Holly, a prickly, spiky flower with white or pale-colored petals\"\n",
      "\"ruby-lipped cattleya\": \"A photo of Ruby-Lipped Cattleya, an epiphytic orchid with long, thin leaves and bright red flowers\"\n",
      "\"cape flower\": \"An image of the Cape Flower, a large, showy flower with bright yellow petals and distinctive markings\"\n",
      "\"great masterwort\": \"A close-up of Great Masterwort, a tall, colorful flower with bright blue or purple markings\"\n",
      "\"siam tulip\": \"A photo of the Siam Tulip, a delicate, cup-shaped flower with bright orange or red petals\"\n",
      "\"lenten rose\": \"An image of Lenten Rose, a small, fragrant flower with delicate, ruffled petals.\"\n"
     ]
    }
   ],
   "source": [
    "# # Print the generated prompts in the required format\n",
    "print(\"Generated Prompts:\")\n",
    "# print(\"========================================\")\n",
    "# for k, v in flower_prompts_dict.items():\n",
    "#     print(f'\"{k}\": \"{v}\"')\n",
    "\n",
    "# import json\n",
    "# # Save the generated prompts to a JSON file\n",
    "# with open('generated_prompts.json', 'w') as f:\n",
    "#     json.dump(flower_prompts_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada793f5",
   "metadata": {},
   "source": [
    "## Loading LLDM model (LLaDa 8B - Instruct) and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106dbf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', \n",
    "                                  trust_remote_code=True, \n",
    "                                  torch_dtype=torch.bfloat16, \n",
    "                                  load_in_4bit=True, # use 4-bit quantization to reduce memory usage\n",
    "                                  device_map=\"auto\" # automatically map model to available devices\n",
    "                                  ).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7e667",
   "metadata": {},
   "source": [
    "â— The function for generating the prompt are based on the official LLaDa's repository (you can check it [here](https://github.com/ML-GSAI/LLaDA/blob/main/generate.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437243c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gumbel_noise(logits, temperature):\n",
    "    '''\n",
    "    The Gumbel max is a method for sampling categorical distributions.\n",
    "    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\n",
    "    Thus, we use float64.\n",
    "    '''\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (- torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    '''\n",
    "    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\n",
    "    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\n",
    "    the expected number of tokens transitioned at each step should be consistent.\n",
    "\n",
    "    This function is designed to precompute the number of tokens that need to be transitioned at each step.\n",
    "    '''\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "\n",
    "    return num_transfer_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f168673b",
   "metadata": {},
   "source": [
    "### Prompt generation via diffusion model\n",
    "\n",
    "The idea is to generate a prompt from a set of tokens set to [MASK], these tokens will iteratively replaced with the most probable tokens given the context of the prompt, the others are kept [MASK]. The process is repeated until the prompt is fully generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ torch.no_grad()\n",
    "def generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,\n",
    "             cfg_scale=0., remasking='low_confidence', mask_id=126336):\n",
    "    '''\n",
    "    Args:\n",
    "        model: Mask predictor.\n",
    "        prompt: A tensor of shape (1, L). Defining what the model is required to generate.\n",
    "        steps: Sampling steps, less than or equal to gen_length.\n",
    "        gen_length: Generated answer length.\n",
    "        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.\n",
    "        temperature: Categorical distribution sampling temperature.\n",
    "        cfg_scale: Unsupervised classifier-free guidance scale.\n",
    "        remasking: Remasking strategy. 'low_confidence' or 'random'.\n",
    "        mask_id: The toke id of [MASK] is 126336.\n",
    "    '''\n",
    "    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)  #! creates a vector with all [MASK] tokens (e.g. [MASK] [MASK] [MASK] ... [MASK])\n",
    "    x[:, :prompt.shape[1]] = prompt.clone()\n",
    "\n",
    "    prompt_index = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0\n",
    "    num_blocks = gen_length // block_length\n",
    "\n",
    "    assert steps % num_blocks == 0\n",
    "    steps = steps // num_blocks\n",
    "\n",
    "    for num_block in range(num_blocks):  #* for each block to be generated we apply a loop of \"steps\" to make the denoising pass (for internal)\n",
    "        block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)  #! calculate the number of tokens to be transferred for each step\n",
    "        for i in range(steps): #* qui fa gli steps di denoising pass per il blocco corrente\n",
    "            mask_index = (x == mask_id)\n",
    "            if cfg_scale > 0.:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = mask_id\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(x_).logits               #! Model prediction for the current block\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x).logits\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature) # add gumbel noise to logits\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1) # b, l    -> #! keep only the most likely tokens\n",
    "\n",
    "\n",
    "            #! the idea here is to reveal some tokens and the others will remain [MASK]\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits, dim=-1)\n",
    "                x0_p = torch.squeeze(\n",
    "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l  -> #* x0_p contains the probability of the token predettto x0 in each position\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(remasking)\n",
    "\n",
    "            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])   #! Select the highest probability tokens\n",
    "                transfer_index[j, select_index] = True       #! reveals the token\n",
    "            x[transfer_index] = x0[transfer_index]   #! the tokens selected with topk are unlocked and revealed, all others remain [MASK] for the next step\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9d9cd",
   "metadata": {},
   "source": [
    "### Prompt generation given a set of classes\n",
    "We pass the set of classes and we generate a prompt for each one uisng ther LLDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35926ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TEST_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1378de",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a vision-language expert. Given a list of flower classes, generate natural, detailed captions that describes what the flower looks like, including its color, petal shape, size, and any distinctive features. Each caption should be written as a single sentence that could be used as a prompt for CLIP.\n",
    "\n",
    "Use the following examples as a guide:\n",
    "\n",
    "Class: Calendula\n",
    "â†’ \"A close-up photo of a Calendula, a bright yellow-orange flower with daisy-like petals and sticky stems;\"\n",
    "\n",
    "Class: Rose\n",
    "â†’ \"A photo of a Rose, a layered flower with soft, velvety petals, often deep red and spiraled in shape;\"\n",
    "\n",
    "Class: Dandelion\n",
    "â†’ \"An image of a Dandelion, a small yellow flower with thin, radiating petals and a fluffy seed head;\"\n",
    "\n",
    "Class: Tulip\n",
    "â†’ \"A photograph of a Tulip, a smooth, cup-shaped flower with bright red petals and long green stems;\"\n",
    "\n",
    "Class: Iris\n",
    "â†’ \"A close-up of an Iris, a violet-purple flower with ruffled petals and a distinctive yellow stripe down the center;\"\n",
    "\n",
    "Please output all the captions in a single line, separated by a semicolon;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(model, classes, gen_length=128, block_length=32, steps=128):\n",
    "    \"\"\"\n",
    "    Generate prompts for a list of classes using the LLaDA model.\n",
    "    Args:\n",
    "        model: The LLaDA model to use for generating prompts.\n",
    "        classes: A list of class names for which to generate prompts.\n",
    "    Returns:\n",
    "        A dictionary where keys are class names and values are generated prompts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unifying all the classes into a single string, that will be the input prompt together with the system prompt\n",
    "    class_string = \", \".join(classes)\n",
    "    \n",
    "    # create the input message for the model\n",
    "    m = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": class_string},]\n",
    "    \n",
    "    # Tokenizing input message\n",
    "    instruction = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "    input_ids = tokenizer(instruction)['input_ids']\n",
    "    input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "    \n",
    "    # Generating prompts\n",
    "    out = generate(model, input_ids, steps=steps, gen_length=gen_length, block_length=block_length, temperature=0.2, cfg_scale=0., remasking='low_confidence')\n",
    "    caption_string = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Prompts are separated by semicolons, so we split them to create a list\n",
    "    extracted_prompts = caption_string.split(';')\n",
    "    extracted_prompts = [prompt.strip() for prompt in extracted_prompts if prompt.strip()]\n",
    "    \n",
    "    # Building dictionary (class name -> prompt)\n",
    "    class_prompt_dict = dict(zip(classes, extracted_prompts))\n",
    "    \n",
    "    return class_prompt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027675e2",
   "metadata": {},
   "source": [
    "# Prompt refinement\n",
    "We select prompts in which CLIP provides similar probabilities and we refine them by masking part of them and generating new tokens trying to be more specific (longer prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e43200",
   "metadata": {},
   "source": [
    "- add +50% length to the output prompt (consider average length of the generated prompts)\n",
    "- mask **each prompt** accoriding a certain probability $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6129c",
   "metadata": {},
   "source": [
    "Note: adapt generate_refined adding the random masking\n",
    "\n",
    "+ ADD a new system prompt called REFINEMENT_SYSTEM_PROMPT in which it's explained that the model will get a series of prompts describing a flower class, it is required to generate more specific prompts useful for CLIP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !! RUN THIS JUST ONCE !! Uncomment if the cell [17] is not run\n",
    "# # !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt_tab')                     # tokenizer\n",
    "# nltk.download('averaged_perceptron_tagger_eng')# POS-tagger usato da pos_tag\n",
    "# nltk.download('universal_tagset')                     # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6422fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def random_mask_tokens(text, p=0.25, keep_keywords=CLASS_NAMES):\n",
    "    \"\"\"\n",
    "    Mask with probability p some tokens but NOT the keywords.\n",
    "    keep_keywords: list of keywords like \"rose\", \"tulip\", etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    words = word_tokenize(text.lower())  # Tokenize the text\n",
    "    pos = pos_tag(words)  # Get POS tags to avoid masking keywords\n",
    "    keep_keywords = set(kw.lower() for kw in keep_keywords)  # Normalize keywords to lowercase\n",
    "    \n",
    "    masked_tokens = []\n",
    "    \n",
    "    for word, tag in pos:\n",
    "        if word in keep_keywords:\n",
    "            masked_tokens.append(word)   # keep unchanged the keywords\n",
    "        elif tag in {'JJ', 'JJR', 'JJS', 'RB', 'VBG', 'VBN'}:\n",
    "            num_of_masks = random.randint(1, 3)\n",
    "            masked_tokens.append(\"[MASK]\"*num_of_masks)  # adjective are masked with two [MASK] tokens\n",
    "        else:\n",
    "            masked_tokens.append(word)  # Keep the token unchanged\n",
    "            \n",
    "    return ' '.join(masked_tokens)  # Join the tokens back into a string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc534ea9",
   "metadata": {},
   "source": [
    "## Testing random masking\n",
    "Randomize some of the generated prompts, so that the model will not always generate the same prompt for the same class.\n",
    "In case prompt dictionary is not defined, we will load it from the file `generated_prompts.json` (if exists) or we will generate it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e687b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not 'prompt_dictionary' in locals()) or (not 'prompt_dictionary' in globals()):\n",
    "    # If the prompt dictionary is not defined, we will load it from the file `generated_prompts.json` (if exists) or we will generate it from scratch.\n",
    "    if os.path.exists('generated_prompts.json'):\n",
    "        import json\n",
    "        with open('generated_prompts.json', 'r') as f:\n",
    "            prompt_dictionary = json.load(f)\n",
    "    else:\n",
    "        prompt_dictionary = generate_prompts(model, [CLASS_NAMES[i] for i in range(15)])\n",
    "        \n",
    "\n",
    "# Randomize the generated prompts\n",
    "randomized_prompt_dict = {}\n",
    "for c, prompt in prompt_dictionary.items():\n",
    "    # Randomize the prompt with a probability of 0.25\n",
    "    randomized_prompt_dict[c] = random_mask_tokens(prompt, p=0.25, keep_keywords=CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a708a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pink primrose': 'a [MASK][MASK][MASK] photo of a pink primrose , a [MASK][MASK] pink flower with [MASK][MASK] petals and a [MASK][MASK][MASK] center',\n",
       " 'hard-leaved pocket orchid': '[MASK][MASK] pocket orchid , a [MASK][MASK][MASK] , [MASK][MASK][MASK] flower with [MASK] petals and a [MASK][MASK] center',\n",
       " 'canterbury bells': 'canterbury bells , a cluster of [MASK][MASK] [MASK][MASK] flowers with [MASK][MASK][MASK] , [MASK][MASK][MASK] petals',\n",
       " 'sweet pea': '[MASK] pea , a [MASK][MASK][MASK] , [MASK][MASK][MASK] flower with [MASK][MASK][MASK] petals and a [MASK][MASK] center',\n",
       " 'english marigold': '[MASK][MASK][MASK] marigold , a [MASK] [MASK][MASK] flower with [MASK][MASK][MASK] , [MASK][MASK][MASK] petals',\n",
       " 'tiger lily': 'tiger [MASK][MASK][MASK] , a [MASK][MASK] , [MASK] flower with [MASK][MASK] petals and a [MASK] center',\n",
       " 'moon orchid': 'moon orchid , a [MASK][MASK][MASK] , [MASK][MASK][MASK] flower with delicate , [MASK][MASK] petals and a [MASK][MASK][MASK] center .'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomized_prompt_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
