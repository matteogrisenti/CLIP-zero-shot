{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60897f0",
   "metadata": {},
   "source": [
    "# Prompt generation\n",
    "\n",
    "This is a temporary notebook in which we will generate the prompt for clip given the set of classes from Flowers102.\n",
    "\n",
    "Specifically, we first generate the prompt from scratch using a LLDM for each class, then we will refine a specific set of prompts given the classes and the previously generated prompts. This latter step will be employed in case CLIP will predict different class with similar probability in order to create a more detailed prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e532365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234bc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3730176",
   "metadata": {},
   "source": [
    "**Loading the classes from Flowers102**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28aa8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4afaa584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_novel_categories(dataset):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb92d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Class Names: [(0, 'pink primrose'), (1, 'hard-leaved pocket orchid'), (2, 'canterbury bells'), (3, 'sweet pea'), (4, 'english marigold'), (5, 'tiger lily'), (6, 'moon orchid'), (7, 'bird of paradise'), (8, 'monkshood'), (9, 'globe thistle'), (10, 'snapdragon'), (11, \"colt's foot\"), (12, 'king protea'), (13, 'spear thistle'), (14, 'yellow iris'), (15, 'globe-flower'), (16, 'purple coneflower'), (17, 'peruvian lily'), (18, 'balloon flower'), (19, 'giant white arum lily'), (20, 'fire lily'), (21, 'pincushion flower'), (22, 'fritillary'), (23, 'red ginger'), (24, 'grape hyacinth'), (25, 'corn poppy'), (26, 'prince of wales feathers'), (27, 'stemless gentian'), (28, 'artichoke'), (29, 'sweet william'), (30, 'carnation'), (31, 'garden phlox'), (32, 'love in the mist'), (33, 'mexican aster'), (34, 'alpine sea holly'), (35, 'ruby-lipped cattleya'), (36, 'cape flower'), (37, 'great masterwort'), (38, 'siam tulip'), (39, 'lenten rose'), (40, 'barbeton daisy'), (41, 'daffodil'), (42, 'sword lily'), (43, 'poinsettia'), (44, 'bolero deep blue'), (45, 'wallflower'), (46, 'marigold'), (47, 'buttercup'), (48, 'oxeye daisy'), (49, 'common dandelion'), (50, 'petunia')]\n",
      "Novel Class Names: [(51, 'wild pansy'), (52, 'primula'), (53, 'sunflower'), (54, 'pelargonium'), (55, 'bishop of llandaff'), (56, 'gaura'), (57, 'geranium'), (58, 'orange dahlia'), (59, 'pink-yellow dahlia?'), (60, 'cautleya spicata'), (61, 'japanese anemone'), (62, 'black-eyed susan'), (63, 'silverbush'), (64, 'californian poppy'), (65, 'osteospermum'), (66, 'spring crocus'), (67, 'bearded iris'), (68, 'windflower'), (69, 'tree poppy'), (70, 'gazania'), (71, 'azalea'), (72, 'water lily'), (73, 'rose'), (74, 'thorn apple'), (75, 'morning glory'), (76, 'passion flower'), (77, 'lotus'), (78, 'toad lily'), (79, 'anthurium'), (80, 'frangipani'), (81, 'clematis'), (82, 'hibiscus'), (83, 'columbine'), (84, 'desert-rose'), (85, 'tree mallow'), (86, 'magnolia'), (87, 'cyclamen'), (88, 'watercress'), (89, 'canna lily'), (90, 'hippeastrum'), (91, 'bee balm'), (92, 'ball moss'), (93, 'foxglove'), (94, 'bougainvillea'), (95, 'camellia'), (96, 'mallow'), (97, 'mexican petunia'), (98, 'bromelia'), (99, 'blanket flower'), (100, 'trumpet creeper'), (101, 'blackberry lily')]\n"
     ]
    }
   ],
   "source": [
    "_, _, tmp_test = get_data()\n",
    "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
    "print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64bb4ae",
   "metadata": {},
   "source": [
    "I will simply use the variable `CLASS_NAMES` as list of classes.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df064d8",
   "metadata": {},
   "source": [
    "## Loading LLDM model (LLaDa 8B - Instruct) and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4d472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disi/miniconda3/envs/lldm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', \n",
    "                                  trust_remote_code=True, \n",
    "                                  torch_dtype=torch.bfloat16, \n",
    "                                  load_in_4bit=True, # use 4-bit quantization to reduce memory usage\n",
    "                                  device_map=\"auto\" # automatically map model to available devices\n",
    "                                  ).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1f5c7",
   "metadata": {},
   "source": [
    "❗ The function for generating the prompt are based on the official LLaDa's repository (you can check it [here](https://github.com/ML-GSAI/LLaDA/blob/main/generate.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9873b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gumbel_noise(logits, temperature):\n",
    "    '''\n",
    "    The Gumbel max is a method for sampling categorical distributions.\n",
    "    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\n",
    "    Thus, we use float64.\n",
    "    '''\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (- torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    '''\n",
    "    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\n",
    "    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\n",
    "    the expected number of tokens transitioned at each step should be consistent.\n",
    "\n",
    "    This function is designed to precompute the number of tokens that need to be transitioned at each step.\n",
    "    '''\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "\n",
    "    return num_transfer_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497d4af",
   "metadata": {},
   "source": [
    "### Prompt generation via diffusion model\n",
    "\n",
    "The idea is to generate a prompt from a set of tokens set to [MASK], these tokens will iteratively replaced with the most probable tokens given the context of the prompt, the others are kept [MASK]. The process is repeated until the prompt is fully generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7076411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ torch.no_grad()\n",
    "def generate(model, prompt, steps=128, gen_length=2000, block_length=2000, temperature=0.,\n",
    "             cfg_scale=0., remasking='low_confidence', mask_id=126336):\n",
    "    '''\n",
    "    Args:\n",
    "        model: Mask predictor.\n",
    "        prompt: A tensor of shape (1, L). Defining what the model is required to generate.\n",
    "        steps: Sampling steps, less than or equal to gen_length.\n",
    "        gen_length: Generated answer length.\n",
    "        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.\n",
    "        temperature: Categorical distribution sampling temperature.\n",
    "        cfg_scale: Unsupervised classifier-free guidance scale.\n",
    "        remasking: Remasking strategy. 'low_confidence' or 'random'.\n",
    "        mask_id: The toke id of [MASK] is 126336.\n",
    "    '''\n",
    "    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)  #! creates a vector with all [MASK] tokens (e.g. [MASK] [MASK] [MASK] ... [MASK])\n",
    "    x[:, :prompt.shape[1]] = prompt.clone()\n",
    "\n",
    "    prompt_index = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0\n",
    "    num_blocks = gen_length // block_length\n",
    "\n",
    "    assert steps % num_blocks == 0\n",
    "    steps = steps // num_blocks\n",
    "\n",
    "    for num_block in range(num_blocks):  #* for each block to be generated we apply a loop of \"steps\" to make the denoising pass (for internal)\n",
    "        block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)  #! calculate the number of tokens to be transferred for each step\n",
    "        for i in range(steps): #* qui fa gli steps di denoising pass per il blocco corrente\n",
    "            mask_index = (x == mask_id)\n",
    "            if cfg_scale > 0.:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = mask_id\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(x_).logits               #! Model prediction for the current block\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x).logits\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature) # add gumbel noise to logits\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1) # b, l    -> #! keep only the most likely tokens\n",
    "\n",
    "\n",
    "            #! the idea here is to reveal some tokens and the others will remain [MASK]\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits, dim=-1)\n",
    "                x0_p = torch.squeeze(\n",
    "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l  -> #* x0_p contains the probability of the token predettto x0 in each position\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(remasking)\n",
    "\n",
    "            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])   #! Select the highest probability tokens\n",
    "                transfer_index[j, select_index] = True       #! reveals the token\n",
    "            x[transfer_index] = x0[transfer_index]   #! the tokens selected with topk are unlocked and revealed, all others remain [MASK] for the next step\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfe4c3",
   "metadata": {},
   "source": [
    "### Prompt generation given a set of classes\n",
    "We pass the set of classes and we generate a prompt for each one uisng ther LLDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfa417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a vision-language expert. Given a list of flower classes, generate natural, detailed captions that describes what the flower looks like, including its color, petal shape, size, and any distinctive features. Each caption should be written as a single sentence that could be used as a prompt for CLIP.\n",
    "\n",
    "Use the following examples as a guide:\n",
    "\n",
    "Class: Calendula\n",
    "→ \"A close-up photo of a Calendula, a bright yellow-orange flower with daisy-like petals and sticky stems.\"\n",
    "\n",
    "Class: Rose\n",
    "→ \"A photo of a Rose, a layered flower with soft, velvety petals, often deep red and spiraled in shape.\"\n",
    "\n",
    "Class: Dandelion\n",
    "→ \"An image of a Dandelion, a small yellow flower with thin, radiating petals and a fluffy seed head.\"\n",
    "\n",
    "Class: Tulip\n",
    "→ \"A photograph of a Tulip, a smooth, cup-shaped flower with bright red petals and long green stems.\"\n",
    "\n",
    "Class: Iris\n",
    "→ \"A close-up of an Iris, a violet-purple flower with ruffled petals and a distinctive yellow stripe down the center.\"\n",
    "\n",
    "Please output all the captions in a single line, separated by a semicolon.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a0084da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(model, classes):\n",
    "    \"\"\"\n",
    "    Generate prompts for a list of classes using the LLaDA model.\n",
    "    Args:\n",
    "        model: The LLaDA model to use for generating prompts.\n",
    "        classes: A list of class names for which to generate prompts.\n",
    "    Returns:\n",
    "        A dictionary where keys are class names and values are generated prompts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unifying all the classes into a single string, that will be the input prompt together with the system prompt\n",
    "    class_string = \", \".join(classes)\n",
    "    \n",
    "    # create the input message for the model\n",
    "    m = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": class_string},]\n",
    "    \n",
    "    # Tokenizing input message\n",
    "    instruction = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "    input_ids = tokenizer(instruction)['input_ids']\n",
    "    input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "    \n",
    "    # Generating prompts\n",
    "    out = generate(model, input_ids, steps=128, gen_length=128, block_length=32, temperature=0., cfg_scale=0., remasking='low_confidence')\n",
    "    caption_string = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Prompts are separated by semicolons, so we split them to create a list\n",
    "    extracted_prompts = caption_string.split(';')\n",
    "    extracted_prompts = [prompt.strip() for prompt in extracted_prompts if prompt.strip()]\n",
    "    \n",
    "    # Building dictionary (class name -> prompt)\n",
    "    class_prompt_dict = dict(zip(classes, extracted_prompts))\n",
    "    \n",
    "    return class_prompt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259057b",
   "metadata": {},
   "source": [
    "# Testing class generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ed113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9c310e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb70e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_dictionary = generate_prompts(model, [CLASS_NAMES[i] for i in range(15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9e9e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the generated prompts in the required format\n",
    "# print(\"Generated Prompts:\")\n",
    "# print(\"========================================\")\n",
    "# for k, v in prompt_dictionary.items():\n",
    "#     print(f'\"{k}\": \"{v}\"')\n",
    "\n",
    "# import json\n",
    "# # Save the generated prompts to a JSON file\n",
    "# with open('generated_prompts.json', 'w') as f:\n",
    "#     json.dump(prompt_dictionary, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027675e2",
   "metadata": {},
   "source": [
    "# Prompt refinement\n",
    "We select prompts in which CLIP provides similar probabilities and we refine them by masking part of them and generating new tokens trying to be more specific (longer prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e43200",
   "metadata": {},
   "source": [
    "- add +50% length to the output prompt (consider average length of the generated prompts)\n",
    "- mask **each prompt** accoriding a certain probability $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6129c",
   "metadata": {},
   "source": [
    "Note: adapt generate_refined adding the random masking\n",
    "\n",
    "+ ADD a new system prompt called REFINEMENT_SYSTEM_PROMPT in which it's explained that the model will get a series of prompts describing a flower class, it is required to generate more specific prompts useful for CLIP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !! RUN THIS JUST ONCE !! Uncomment if the cell [17] is not run\n",
    "# # !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt_tab')                     # tokenizer\n",
    "# nltk.download('averaged_perceptron_tagger_eng')# POS-tagger usato da pos_tag\n",
    "# nltk.download('universal_tagset')                     # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6422fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def random_mask_tokens(text, p=0.25, keep_keywords=CLASS_NAMES):\n",
    "    \"\"\"\n",
    "    Mask with probability p some tokens but NOT the keywords.\n",
    "    keep_keywords: list of keywords like \"rose\", \"tulip\", etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    words = word_tokenize(text.lower())  # Tokenize the text\n",
    "    pos = pos_tag(words)  # Get POS tags to avoid masking keywords\n",
    "    keep_keywords = set(kw.lower() for kw in keep_keywords)  # Normalize keywords to lowercase\n",
    "    \n",
    "    masked_tokens = []\n",
    "    \n",
    "    for word, tag in pos:\n",
    "        if word in keep_keywords:\n",
    "            masked_tokens.append(word)   # keep unchanged the keywords\n",
    "        elif tag in {'JJ', 'JJR', 'JJS', 'RB', 'VBG', 'VBN'}:\n",
    "            num_of_masks = random.randint(1, 3)\n",
    "            masked_tokens.append(\"[MASK]\"*num_of_masks)  # adjective are masked with two [MASK] tokens\n",
    "        else:\n",
    "            masked_tokens.append(word)  # Keep the token unchanged\n",
    "            \n",
    "    return ' '.join(masked_tokens)  # Join the tokens back into a string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc534ea9",
   "metadata": {},
   "source": [
    "## Testing random masking\n",
    "Randomize some of the generated prompts, so that the model will not always generate the same prompt for the same class.\n",
    "In case prompt dictionary is not defined, we will load it from the file `generated_prompts.json` (if exists) or we will generate it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e687b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not 'prompt_dictionary' in locals()) or (not 'prompt_dictionary' in globals()):\n",
    "    # If the prompt dictionary is not defined, we will load it from the file `generated_prompts.json` (if exists) or we will generate it from scratch.\n",
    "    if os.path.exists('generated_prompts.json'):\n",
    "        import json\n",
    "        with open('generated_prompts.json', 'r') as f:\n",
    "            prompt_dictionary = json.load(f)\n",
    "    else:\n",
    "        prompt_dictionary = generate_prompts(model, [CLASS_NAMES[i] for i in range(15)])\n",
    "        \n",
    "\n",
    "# Randomize the generated prompts\n",
    "randomized_prompt_dict = {}\n",
    "for c, prompt in prompt_dictionary.items():\n",
    "    # Randomize the prompt with a probability of 0.25\n",
    "    randomized_prompt_dict[c] = random_mask_tokens(prompt, p=0.25, keep_keywords=CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d6a708a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pink primrose': 'a [MASK][MASK][MASK] photo of a pink primrose , a [MASK][MASK] pink flower with [MASK][MASK] petals and a [MASK][MASK][MASK] center',\n",
       " 'hard-leaved pocket orchid': '[MASK][MASK] pocket orchid , a [MASK][MASK][MASK] , [MASK][MASK][MASK] flower with [MASK] petals and a [MASK][MASK] center',\n",
       " 'canterbury bells': 'canterbury bells , a cluster of [MASK][MASK] [MASK][MASK] flowers with [MASK][MASK][MASK] , [MASK][MASK][MASK] petals',\n",
       " 'sweet pea': '[MASK] pea , a [MASK][MASK][MASK] , [MASK][MASK][MASK] flower with [MASK][MASK][MASK] petals and a [MASK][MASK] center',\n",
       " 'english marigold': '[MASK][MASK][MASK] marigold , a [MASK] [MASK][MASK] flower with [MASK][MASK][MASK] , [MASK][MASK][MASK] petals',\n",
       " 'tiger lily': 'tiger [MASK][MASK][MASK] , a [MASK][MASK] , [MASK] flower with [MASK][MASK] petals and a [MASK] center',\n",
       " 'moon orchid': 'moon orchid , a [MASK][MASK][MASK] , [MASK][MASK][MASK] flower with delicate , [MASK][MASK] petals and a [MASK][MASK][MASK] center .'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomized_prompt_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
