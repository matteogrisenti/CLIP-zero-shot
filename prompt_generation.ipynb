{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60897f0",
   "metadata": {},
   "source": [
    "# Prompt generation\n",
    "\n",
    "This is a temporary notebook in which we will generate the prompt for clip given the set of classes from Flowers102.\n",
    "\n",
    "Specifically, we first generate the prompt from scratch using a LLDM for each class, then we will refine a specific set of prompts given the classes and the previously generated prompts. This latter step will be employed in case CLIP will predict different class with similar probability in order to create a more detailed prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e532365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "234bc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3730176",
   "metadata": {},
   "source": [
    "**Loading the classes from Flowers102**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28aa8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4afaa584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_novel_categories(dataset):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb92d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Class Names: [(0, 'pink primrose'), (1, 'hard-leaved pocket orchid'), (2, 'canterbury bells'), (3, 'sweet pea'), (4, 'english marigold'), (5, 'tiger lily'), (6, 'moon orchid'), (7, 'bird of paradise'), (8, 'monkshood'), (9, 'globe thistle'), (10, 'snapdragon'), (11, \"colt's foot\"), (12, 'king protea'), (13, 'spear thistle'), (14, 'yellow iris'), (15, 'globe-flower'), (16, 'purple coneflower'), (17, 'peruvian lily'), (18, 'balloon flower'), (19, 'giant white arum lily'), (20, 'fire lily'), (21, 'pincushion flower'), (22, 'fritillary'), (23, 'red ginger'), (24, 'grape hyacinth'), (25, 'corn poppy'), (26, 'prince of wales feathers'), (27, 'stemless gentian'), (28, 'artichoke'), (29, 'sweet william'), (30, 'carnation'), (31, 'garden phlox'), (32, 'love in the mist'), (33, 'mexican aster'), (34, 'alpine sea holly'), (35, 'ruby-lipped cattleya'), (36, 'cape flower'), (37, 'great masterwort'), (38, 'siam tulip'), (39, 'lenten rose'), (40, 'barbeton daisy'), (41, 'daffodil'), (42, 'sword lily'), (43, 'poinsettia'), (44, 'bolero deep blue'), (45, 'wallflower'), (46, 'marigold'), (47, 'buttercup'), (48, 'oxeye daisy'), (49, 'common dandelion'), (50, 'petunia')]\n",
      "Novel Class Names: [(51, 'wild pansy'), (52, 'primula'), (53, 'sunflower'), (54, 'pelargonium'), (55, 'bishop of llandaff'), (56, 'gaura'), (57, 'geranium'), (58, 'orange dahlia'), (59, 'pink-yellow dahlia?'), (60, 'cautleya spicata'), (61, 'japanese anemone'), (62, 'black-eyed susan'), (63, 'silverbush'), (64, 'californian poppy'), (65, 'osteospermum'), (66, 'spring crocus'), (67, 'bearded iris'), (68, 'windflower'), (69, 'tree poppy'), (70, 'gazania'), (71, 'azalea'), (72, 'water lily'), (73, 'rose'), (74, 'thorn apple'), (75, 'morning glory'), (76, 'passion flower'), (77, 'lotus'), (78, 'toad lily'), (79, 'anthurium'), (80, 'frangipani'), (81, 'clematis'), (82, 'hibiscus'), (83, 'columbine'), (84, 'desert-rose'), (85, 'tree mallow'), (86, 'magnolia'), (87, 'cyclamen'), (88, 'watercress'), (89, 'canna lily'), (90, 'hippeastrum'), (91, 'bee balm'), (92, 'ball moss'), (93, 'foxglove'), (94, 'bougainvillea'), (95, 'camellia'), (96, 'mallow'), (97, 'mexican petunia'), (98, 'bromelia'), (99, 'blanket flower'), (100, 'trumpet creeper'), (101, 'blackberry lily')]\n"
     ]
    }
   ],
   "source": [
    "_, _, tmp_test = get_data()\n",
    "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
    "print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64bb4ae",
   "metadata": {},
   "source": [
    "I will simply use the variable `CLASS_NAMES` as list of classes.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df064d8",
   "metadata": {},
   "source": [
    "## Loading LLDM model (LLaDa 8B - Instruct) and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be4d472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', \n",
    "                                  trust_remote_code=True, \n",
    "                                  torch_dtype=torch.bfloat16, \n",
    "                                  load_in_4bit=True, # use 4-bit quantization to reduce memory usage\n",
    "                                  device_map=\"auto\" # automatically map model to available devices\n",
    "                                  ).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1f5c7",
   "metadata": {},
   "source": [
    "❗ The function for generating the prompt are based on the official LLaDa's repository (you can check it [here](https://github.com/ML-GSAI/LLaDA/blob/main/generate.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9873b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gumbel_noise(logits, temperature):\n",
    "    '''\n",
    "    The Gumbel max is a method for sampling categorical distributions.\n",
    "    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\n",
    "    Thus, we use float64.\n",
    "    '''\n",
    "    if temperature == 0:\n",
    "        return logits\n",
    "    logits = logits.to(torch.float64)\n",
    "    noise = torch.rand_like(logits, dtype=torch.float64)\n",
    "    gumbel_noise = (- torch.log(noise)) ** temperature\n",
    "    return logits.exp() / gumbel_noise\n",
    "\n",
    "\n",
    "def get_num_transfer_tokens(mask_index, steps):\n",
    "    '''\n",
    "    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\n",
    "    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\n",
    "    the expected number of tokens transitioned at each step should be consistent.\n",
    "\n",
    "    This function is designed to precompute the number of tokens that need to be transitioned at each step.\n",
    "    '''\n",
    "    mask_num = mask_index.sum(dim=1, keepdim=True)\n",
    "\n",
    "    base = mask_num // steps\n",
    "    remainder = mask_num % steps\n",
    "\n",
    "    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n",
    "\n",
    "    for i in range(mask_num.size(0)):\n",
    "        num_transfer_tokens[i, :remainder[i]] += 1\n",
    "\n",
    "    return num_transfer_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497d4af",
   "metadata": {},
   "source": [
    "### Prompt generation via diffusion model\n",
    "\n",
    "The idea is to generate a prompt from a set of tokens set to [MASK], these tokens will iteratively replaced with the most probable tokens given the context of the prompt, the others are kept [MASK]. The process is repeated until the prompt is fully generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7076411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ torch.no_grad()\n",
    "def generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,\n",
    "             cfg_scale=0., remasking='low_confidence', mask_id=126336):\n",
    "    '''\n",
    "    Args:\n",
    "        model: Mask predictor.\n",
    "        prompt: A tensor of shape (1, L). Defining what the model is required to generate.\n",
    "        steps: Sampling steps, less than or equal to gen_length.\n",
    "        gen_length: Generated answer length.\n",
    "        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.\n",
    "        temperature: Categorical distribution sampling temperature.\n",
    "        cfg_scale: Unsupervised classifier-free guidance scale.\n",
    "        remasking: Remasking strategy. 'low_confidence' or 'random'.\n",
    "        mask_id: The toke id of [MASK] is 126336.\n",
    "    '''\n",
    "    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)  #! creates a vector with all [MASK] tokens (e.g. [MASK] [MASK] [MASK] ... [MASK])\n",
    "    x[:, :prompt.shape[1]] = prompt.clone()\n",
    "\n",
    "    prompt_index = (x != mask_id)\n",
    "\n",
    "    assert gen_length % block_length == 0\n",
    "    num_blocks = gen_length // block_length\n",
    "\n",
    "    assert steps % num_blocks == 0\n",
    "    steps = steps // num_blocks\n",
    "\n",
    "    for num_block in range(num_blocks):  #* for each block to be generated we apply a loop of \"steps\" to make the denoising pass (for internal)\n",
    "        block_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\n",
    "        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)  #! calculate the number of tokens to be transferred for each step\n",
    "        for i in range(steps): #* qui fa gli steps di denoising pass per il blocco corrente\n",
    "            mask_index = (x == mask_id)\n",
    "            if cfg_scale > 0.:\n",
    "                un_x = x.clone()\n",
    "                un_x[prompt_index] = mask_id\n",
    "                x_ = torch.cat([x, un_x], dim=0)\n",
    "                logits = model(x_).logits               #! Model prediction for the current block\n",
    "                logits, un_logits = torch.chunk(logits, 2, dim=0)\n",
    "                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)\n",
    "            else:\n",
    "                logits = model(x).logits\n",
    "\n",
    "            logits_with_noise = add_gumbel_noise(logits, temperature=temperature) # add gumbel noise to logits\n",
    "            x0 = torch.argmax(logits_with_noise, dim=-1) # b, l    -> #! keep only the most likely tokens\n",
    "\n",
    "\n",
    "            #! the idea here is to reveal some tokens and the others will remain [MASK]\n",
    "            if remasking == 'low_confidence':\n",
    "                p = F.softmax(logits, dim=-1)\n",
    "                x0_p = torch.squeeze(\n",
    "                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l  -> #* x0_p contains the probability of the token predettto x0 in each position\n",
    "            elif remasking == 'random':\n",
    "                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\n",
    "            else:\n",
    "                raise NotImplementedError(remasking)\n",
    "\n",
    "            x0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\n",
    "\n",
    "            x0 = torch.where(mask_index, x0, x)\n",
    "            confidence = torch.where(mask_index, x0_p, -np.inf)\n",
    "\n",
    "            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\n",
    "            for j in range(confidence.shape[0]):\n",
    "                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])   #! Select the highest probability tokens\n",
    "                transfer_index[j, select_index] = True       #! reveals the token\n",
    "            x[transfer_index] = x0[transfer_index]   #! the tokens selected with topk are unlocked and revealed, all others remain [MASK] for the next step\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cfe4c3",
   "metadata": {},
   "source": [
    "### Prompt generation given a set of classes\n",
    "We pass the set of classes and we generate a prompt for each one uisng ther LLDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d2cc74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TEST_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6dfa417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a vision-language expert. Given a list of flower classes, generate natural, detailed captions that describes what the flower looks like, including its color, petal shape, size, and any distinctive features. Each caption should be written as a single sentence that could be used as a prompt for CLIP.\n",
    "\n",
    "Use the following examples as a guide:\n",
    "\n",
    "Class: Calendula\n",
    "→ \"A close-up photo of a Calendula, a bright yellow-orange flower with daisy-like petals and sticky stems;\"\n",
    "\n",
    "Class: Rose\n",
    "→ \"A photo of a Rose, a layered flower with soft, velvety petals, often deep red and spiraled in shape;\"\n",
    "\n",
    "Class: Dandelion\n",
    "→ \"An image of a Dandelion, a small yellow flower with thin, radiating petals and a fluffy seed head;\"\n",
    "\n",
    "Class: Tulip\n",
    "→ \"A photograph of a Tulip, a smooth, cup-shaped flower with bright red petals and long green stems;\"\n",
    "\n",
    "Class: Iris\n",
    "→ \"A close-up of an Iris, a violet-purple flower with ruffled petals and a distinctive yellow stripe down the center;\"\n",
    "\n",
    "Please output all the captions in a single line, separated by a semicolon;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6a0084da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(model, classes, gen_length=128, block_length=32, steps=128):\n",
    "    \"\"\"\n",
    "    Generate prompts for a list of classes using the LLaDA model.\n",
    "    Args:\n",
    "        model: The LLaDA model to use for generating prompts.\n",
    "        classes: A list of class names for which to generate prompts.\n",
    "    Returns:\n",
    "        A dictionary where keys are class names and values are generated prompts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unifying all the classes into a single string, that will be the input prompt together with the system prompt\n",
    "    class_string = \", \".join(classes)\n",
    "    \n",
    "    # create the input message for the model\n",
    "    m = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": class_string},]\n",
    "    \n",
    "    # Tokenizing input message\n",
    "    instruction = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "    input_ids = tokenizer(instruction)['input_ids']\n",
    "    input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\n",
    "    \n",
    "    # Generating prompts\n",
    "    out = generate(model, input_ids, steps=steps, gen_length=gen_length, block_length=block_length, temperature=0.2, cfg_scale=0., remasking='low_confidence')\n",
    "    caption_string = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Prompts are separated by semicolons, so we split them to create a list\n",
    "    extracted_prompts = caption_string.split(';')\n",
    "    extracted_prompts = [prompt.strip() for prompt in extracted_prompts if prompt.strip()]\n",
    "    \n",
    "    # Building dictionary (class name -> prompt)\n",
    "    class_prompt_dict = dict(zip(classes, extracted_prompts))\n",
    "    \n",
    "    return class_prompt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259057b",
   "metadata": {},
   "source": [
    "# Testing class generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fb70e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dictionary = generate_prompts(model, [CLASS_NAMES[i] for i in range(N_TEST_CLASSES)], gen_length=256, block_length=64, steps=128) # more ore less 15 tokens per class, block_length=gen_length/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b9e9e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompts:\n",
      "========================================\n",
      "\"pink primrose\": \"A close-up photo of a pink primrose, a delicate flower with soft, velvety pink petals and a subtle fragrance\"\n",
      "\"hard-leaved pocket orchid\": \"A photo of a hard-leaved hard-leaved pocket orchid, a small, delicate flower with with thin, white petals and a smooth, glossy texture\"\n",
      "\"canterbury bells\": \"An image of canterbury bells, a cluster of small, white flowers with delicate, bell-shaped petals and a soft, velvety texture\"\n",
      "\"sweet pea\": \"A close-up of a sweet pea, a small, white flower with delicate, pea-shaped petals and a sweet, cup-like shape\"\n",
      "\"english marigold\": \"A photograph of an english marigold, a bright yellow flower with long, slender petals and a smooth, glossy texture\"\n",
      "\"tiger lily\": \"An image of a tiger lily, a large, orange flower with long, slender petals and a smooth, glossy texture\"\n",
      "\"moon orchid\": \"A close-up of a moon orchid, a small, delicate flower with delicate, white petals and a smooth, glossy texture\"\n",
      "\"bird of paradise\": \"A photo of a bird of paradise, a large, colorful flower with long, slender petals and a subtle fragrance\"\n",
      "\"monkshood\": \"An image of a monkshood, a small, white flower with delicate, bell-shaped petals\"\n",
      "\"globe thistle\": \"A photograph of a globe thistle, a large, white flower with long, slender petals and a subtle fragrance.\"\n"
     ]
    }
   ],
   "source": [
    "# Print the generated prompts in the required format\n",
    "print(\"Generated Prompts:\")\n",
    "print(\"========================================\")\n",
    "for k, v in prompt_dictionary.items():\n",
    "    print(f'\"{k}\": \"{v}\"')\n",
    "\n",
    "import json\n",
    "# Save the generated prompts to a JSON file\n",
    "with open('generated_prompts.json', 'w') as f:\n",
    "    json.dump(prompt_dictionary, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe84b5",
   "metadata": {},
   "source": [
    "### Testing with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "986edf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not 'prompt_dictionary' in locals()) or (not 'prompt_dictionary' in globals()):\n",
    "    # If the prompt dictionary is not defined, we will load it from the file `generated_prompts.json` (if exists) or we will generate it from scratch.\n",
    "    if os.path.exists('generated_prompts.json'):\n",
    "        import json\n",
    "        with open('generated_prompts.json', 'r') as f:\n",
    "            prompt_dictionary = json.load(f)\n",
    "    else:\n",
    "        prompt_dictionary = generate_prompts(model, [CLASS_NAMES[i] for i in range(N_TEST_CLASSES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ed27576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install --yes -c pytorch pytorch=2.7.1 torchvision cudatoolkit=12.8\n",
    "# !pip install ftfy regex tqdm\n",
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bed614f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7faddabf7b80>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# preprocess contains CLIP's pre-defined augmentations, let's inspect them!\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8939c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets select just sample from classes of the generated prompts\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "my_category_samples = []\n",
    "for sample_id, label in enumerate(train_set._labels):\n",
    "    if CLASS_NAMES[label] in prompt_dictionary.keys():\n",
    "        my_category_samples.append(sample_id)\n",
    "        \n",
    "my_train_set = torch.utils.data.Subset(train_set, my_category_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30224e3f",
   "metadata": {},
   "source": [
    "### Classic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a05684bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Zero-shot evaluation on Base Classes:   0%|          | 0/1 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 10 predicted as moon orchid, but it is hard-leaved pocket orchid\n",
      "Image 11 predicted as moon orchid, but it is hard-leaved pocket orchid\n",
      "Image 12 predicted as moon orchid, but it is hard-leaved pocket orchid\n",
      "Image 13 predicted as moon orchid, but it is hard-leaved pocket orchid\n",
      "Image 14 predicted as moon orchid, but it is hard-leaved pocket orchid\n",
      "Image 15 predicted as moon orchid, but it is hard-leaved pocket orchid\n",
      "Image 16 predicted as moon orchid, but it is hard-leaved pocket orchid\n",
      "Image 17 predicted as moon orchid, but it is hard-leaved pocket orchid\n",
      "Image 18 predicted as moon orchid, but it is hard-leaved pocket orchid\n",
      "Image 19 predicted as moon orchid, but it is hard-leaved pocket orchid\n",
      "Image 20 predicted as sweet pea, but it is canterbury bells\n",
      "Image 21 predicted as sweet pea, but it is canterbury bells\n",
      "Image 25 predicted as pink primrose, but it is canterbury bells\n",
      "Image 26 predicted as hard-leaved pocket orchid, but it is canterbury bells\n",
      "Image 28 predicted as pink primrose, but it is canterbury bells\n",
      "Image 33 predicted as canterbury bells, but it is sweet pea\n",
      "Image 35 predicted as pink primrose, but it is sweet pea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Base classes accuracy: 83.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # we don't want gradients\n",
    "def eval(clip_model, dataset, categories, batch_size, device, label=\"\"):\n",
    "    # let's set the model in evaluation mode\n",
    "    clip_model.eval()\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    \n",
    "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
    "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
    "    text_inputs = clip.tokenize(\n",
    "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
    "    ).to(device)\n",
    "\n",
    "    # we can encode the text features once as they are shared for all images\n",
    "    # therefore we do it outside the evaluation loop\n",
    "    text_features = clip_model.encode_text(text_inputs)\n",
    "    # and here we normalize them (standard pratice with CLIP)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward image through CLIP image encoder\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item() \n",
    "        \n",
    "        for i in range(len(predicted_class != target)):\n",
    "            if not predicted_class[i] == target[i]:\n",
    "                print(f\"Image {i} predicted as {CLASS_NAMES[predicted_class[i]]}, but it is {CLASS_NAMES[target[i]]}\")\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = eval(clip_model=clip_model, dataset=my_train_set, categories=[i for i in range(N_TEST_CLASSES)], batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Base Classes\")\n",
    "\n",
    "print()\n",
    "print(f\"🔍 Base classes accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed925176",
   "metadata": {},
   "source": [
    "### Out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0a59dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Zero-shot evaluation on Base Classes:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 11, Wrong prediction: moon orchid (predicted) vs hard-leaved pocket orchid (target)\n",
      "Image: 12, Wrong prediction: moon orchid (predicted) vs hard-leaved pocket orchid (target)\n",
      "Image: 13, Wrong prediction: moon orchid (predicted) vs hard-leaved pocket orchid (target)\n",
      "Image: 14, Wrong prediction: moon orchid (predicted) vs hard-leaved pocket orchid (target)\n",
      "Image: 17, Wrong prediction: moon orchid (predicted) vs hard-leaved pocket orchid (target)\n",
      "Image: 18, Wrong prediction: moon orchid (predicted) vs hard-leaved pocket orchid (target)\n",
      "Image: 20, Wrong prediction: sweet pea (predicted) vs canterbury bells (target)\n",
      "Image: 21, Wrong prediction: sweet pea (predicted) vs canterbury bells (target)\n",
      "Image: 22, Wrong prediction: monkshood (predicted) vs canterbury bells (target)\n",
      "Image: 25, Wrong prediction: pink primrose (predicted) vs canterbury bells (target)\n",
      "Image: 26, Wrong prediction: pink primrose (predicted) vs canterbury bells (target)\n",
      "Image: 28, Wrong prediction: pink primrose (predicted) vs canterbury bells (target)\n",
      "Image: 33, Wrong prediction: canterbury bells (predicted) vs sweet pea (target)\n",
      "Image: 35, Wrong prediction: pink primrose (predicted) vs sweet pea (target)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Base classes accuracy: 86.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # we don't want gradients\n",
    "def eval(clip_model, dataset, prompt_dict, batch_size, device, label=\"\"):\n",
    "    # let's set the model in evaluation mode\n",
    "    clip_model.eval()\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {idx: idx for idx, cat in enumerate(prompt_dict.keys())}\n",
    "    \n",
    "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
    "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
    "    text_inputs = clip.tokenize(\n",
    "        [prompt for prompt in prompt_dict.values()]\n",
    "    ).to(device)\n",
    "\n",
    "    # we can encode the text features once as they are shared for all images\n",
    "    # therefore we do it outside the evaluation loop\n",
    "    text_features = clip_model.encode_text(text_inputs)\n",
    "    # and here we normalize them (standard pratice with CLIP)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward image through CLIP image encoder\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "        \n",
    "        if (predicted_class != target).any():\n",
    "            # if there are any wrong predictions, print them\n",
    "            for i in range(len(predicted_class)):\n",
    "                if predicted_class[i] != target[i]:\n",
    "                    print(f\"Image: {i}, Wrong prediction: {CLASS_NAMES[predicted_class[i]]} (predicted) vs {CLASS_NAMES[target[i]]} (target)\")\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = eval(clip_model=clip_model, dataset=my_train_set, prompt_dict=prompt_dictionary, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Base Classes\")\n",
    "\n",
    "print()\n",
    "print(f\"🔍 Base classes accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027675e2",
   "metadata": {},
   "source": [
    "# Prompt refinement\n",
    "We select prompts in which CLIP provides similar probabilities and we refine them by masking part of them and generating new tokens trying to be more specific (longer prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e43200",
   "metadata": {},
   "source": [
    "- add +50% length to the output prompt (consider average length of the generated prompts)\n",
    "- mask **each prompt** accoriding a certain probability $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6129c",
   "metadata": {},
   "source": [
    "Note: adapt generate_refined adding the random masking\n",
    "\n",
    "+ ADD a new system prompt called REFINEMENT_SYSTEM_PROMPT in which it's explained that the model will get a series of prompts describing a flower class, it is required to generate more specific prompts useful for CLIP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !! RUN THIS JUST ONCE !! Uncomment if the cell [17] is not run\n",
    "# # !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt_tab')                     # tokenizer\n",
    "# nltk.download('averaged_perceptron_tagger_eng')# POS-tagger usato da pos_tag\n",
    "# nltk.download('universal_tagset')                     # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6422fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def random_mask_tokens(text, p=0.25, keep_keywords=CLASS_NAMES):\n",
    "    \"\"\"\n",
    "    Mask with probability p some tokens but NOT the keywords.\n",
    "    keep_keywords: list of keywords like \"rose\", \"tulip\", etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    words = word_tokenize(text.lower())  # Tokenize the text\n",
    "    pos = pos_tag(words)  # Get POS tags to avoid masking keywords\n",
    "    keep_keywords = set(kw.lower() for kw in keep_keywords)  # Normalize keywords to lowercase\n",
    "    \n",
    "    masked_tokens = []\n",
    "    \n",
    "    for word, tag in pos:\n",
    "        if word in keep_keywords:\n",
    "            masked_tokens.append(word)   # keep unchanged the keywords\n",
    "        elif tag in {'JJ', 'JJR', 'JJS', 'RB', 'VBG', 'VBN'}:\n",
    "            num_of_masks = random.randint(1, 3)\n",
    "            masked_tokens.append(\"[MASK]\"*num_of_masks)  # adjective are masked with two [MASK] tokens\n",
    "        else:\n",
    "            masked_tokens.append(word)  # Keep the token unchanged\n",
    "            \n",
    "    return ' '.join(masked_tokens)  # Join the tokens back into a string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc534ea9",
   "metadata": {},
   "source": [
    "## Testing random masking\n",
    "Randomize some of the generated prompts, so that the model will not always generate the same prompt for the same class.\n",
    "In case prompt dictionary is not defined, we will load it from the file `generated_prompts.json` (if exists) or we will generate it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e687b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not 'prompt_dictionary' in locals()) or (not 'prompt_dictionary' in globals()):\n",
    "    # If the prompt dictionary is not defined, we will load it from the file `generated_prompts.json` (if exists) or we will generate it from scratch.\n",
    "    if os.path.exists('generated_prompts.json'):\n",
    "        import json\n",
    "        with open('generated_prompts.json', 'r') as f:\n",
    "            prompt_dictionary = json.load(f)\n",
    "    else:\n",
    "        prompt_dictionary = generate_prompts(model, [CLASS_NAMES[i] for i in range(15)])\n",
    "        \n",
    "\n",
    "# Randomize the generated prompts\n",
    "randomized_prompt_dict = {}\n",
    "for c, prompt in prompt_dictionary.items():\n",
    "    # Randomize the prompt with a probability of 0.25\n",
    "    randomized_prompt_dict[c] = random_mask_tokens(prompt, p=0.25, keep_keywords=CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a708a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pink primrose': 'a [MASK][MASK][MASK] photo of a pink primrose , a [MASK][MASK] pink flower with [MASK][MASK] petals and a [MASK][MASK][MASK] center',\n",
       " 'hard-leaved pocket orchid': '[MASK][MASK] pocket orchid , a [MASK][MASK][MASK] , [MASK][MASK][MASK] flower with [MASK] petals and a [MASK][MASK] center',\n",
       " 'canterbury bells': 'canterbury bells , a cluster of [MASK][MASK] [MASK][MASK] flowers with [MASK][MASK][MASK] , [MASK][MASK][MASK] petals',\n",
       " 'sweet pea': '[MASK] pea , a [MASK][MASK][MASK] , [MASK][MASK][MASK] flower with [MASK][MASK][MASK] petals and a [MASK][MASK] center',\n",
       " 'english marigold': '[MASK][MASK][MASK] marigold , a [MASK] [MASK][MASK] flower with [MASK][MASK][MASK] , [MASK][MASK][MASK] petals',\n",
       " 'tiger lily': 'tiger [MASK][MASK][MASK] , a [MASK][MASK] , [MASK] flower with [MASK][MASK] petals and a [MASK] center',\n",
       " 'moon orchid': 'moon orchid , a [MASK][MASK][MASK] , [MASK][MASK][MASK] flower with delicate , [MASK][MASK] petals and a [MASK][MASK][MASK] center .'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomized_prompt_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
