{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60897f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# PROMPT & IMAGE AUGMENTATION\n",
    "\n",
    "This is a temporary notebook in which we will generate the prompt for clip given the set of classes from Flowers102."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e532365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (4.67.1)\n",
      "Collecting openai_clip\n",
      "  Using cached openai_clip-1.0.1-py3-none-any.whl\n",
      "Collecting ftfy (from openai_clip)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.12/site-packages (from openai_clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from openai_clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from ftfy->openai_clip) (0.2.13)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: ftfy, openai_clip\n",
      "Successfully installed ftfy-6.3.1 openai_clip-1.0.1\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "# ! pip install -q -U google-genai\n",
    "# ! pip install tqdm\n",
    "# ! pip install openai_clip\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3730176",
   "metadata": {},
   "source": [
    "**LIST of FLOWERS**\n",
    "First we define the label name of all the 102 flower classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2f6a4f-f12b-4401-a75c-8d4be1648a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \n",
    "               \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \n",
    "               \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \n",
    "               \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \n",
    "               \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \n",
    "               \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \n",
    "               \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \n",
    "               \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \n",
    "               \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \n",
    "               \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \n",
    "               \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \n",
    "               \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \n",
    "               \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \n",
    "               \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \n",
    "               \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \n",
    "               \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \n",
    "               \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\",\n",
    "               \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \n",
    "               \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \n",
    "               \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \n",
    "               \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \n",
    "               \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "CLASS_NAMES_12 = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \n",
    "               \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \n",
    "               \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52656ec6",
   "metadata": {},
   "source": [
    "### Prompt Generation\n",
    "Here we ask throught API to Google Gemini to derive N prompts for each class. We define a clear prompt for Gemini where we define the number of prompts for each class and define a structured form of the output to better menage it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "928f4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_VARIANTS = 10\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are a vision-language expert.\n",
    "\n",
    "Input format\n",
    "• The user will give one Oxford-102 flower class name (e.g. “Daffodil”).\n",
    "\n",
    "Output format\n",
    "• Return **exactly {NUM_OF_VARIANTS}** short, visually grounded prompts.\n",
    "• Separate the prompts with **“; ”** (semicolon + space).\n",
    "• Do **not** add a semicolon after the final prompt.\n",
    "• Output nothing else.\n",
    "\n",
    "Content rules for each prompt\n",
    "1. Start with a different prefix chosen from this set (use each at most once):\n",
    "   “A photo of a”, “An image of a”, “A close-up of a”, “A macro shot of a”,\n",
    "   “A studio photo of a”, “An outdoor photo of a”, “A botanical plate of a”,\n",
    "   “A well-lit photograph of a”, “A side view of a”, “A top-down view of a”.\n",
    "2. Immediately include the **number of flowers** in the frame (e.g. “single”, “one”, “pair of”, “three”).\n",
    "3. Name the flower class exactly as given.\n",
    "4. Mention dominant colour(s) plus 1-2 key visual cues (petal count/shape, centre, leaves, stem).\n",
    "5. Keep each prompt ≤ 25 words.\n",
    "6. Use only information visible in a typical image; avoid subjective or non-visual terms (beautiful, symbolic, fragrant, etc.).\n",
    "7. End each prompt **without** a period.\n",
    "\n",
    "Return the ten prompts in one line, separated by “; ”, and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0736947d-0f05-4c96-a7fb-368d3ec0b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# GEMINI API KEY IN SECRET's SECTION (left menu)\n",
    "''' \n",
    "# If noteboock in Google Colab\n",
    "from google.colab import userdata\n",
    "userdata.get('GEMINI_API_KEY')\n",
    "'''\n",
    "\n",
    "# If noteboock in Jupiter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "# print(gemini_api_key)\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "525258ab-7d6f-4bad-8c7f-8f1e6cd169b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classes_prompt_variants(classes: list):\n",
    "    '''\n",
    "    This function generate a set of N prompts for each class in the input list parameter\n",
    "    '''\n",
    "    prompt_variants_dict = {}  \n",
    "\n",
    "    # Iterate each class: i is the class number, c is the class label\n",
    "    for i, c in enumerate(classes):\n",
    "        try:\n",
    "            # Send the request to gemini with API\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                config=types.GenerateContentConfig(\n",
    "                    system_instruction=SYSTEM_PROMPT,\n",
    "                    temperature=0.1\n",
    "                ),\n",
    "                contents=c\n",
    "            )\n",
    "            # Extract the response\n",
    "            all_prompts = response.text\n",
    "            prompt_variants_dict[c] = all_prompts.split('; ')\n",
    "            print(f\"✅ Finished class {c} - {i+1}/{len(classes)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating class '{c}' - {i+1}/{len(classes)}: {e}\")\n",
    "            prompt_variants_dict[c] = []  # oppure `None`, a seconda di cosa preferisci\n",
    "\n",
    "    return prompt_variants_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3bd47-dbc2-47f1-9ac5-ad3d5bdf2113",
   "metadata": {},
   "source": [
    "We initialize our system by define the prompts for each of the 102 classes of our database. Either this the code is orgnaized in order to flexible and easy to generate new prompts for new classes.\n",
    "\n",
    "To avoid overload Gemnin and risk to get lower quality result we devide the classes in small batches of 6 classes each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9004ba11-e4d4-48bb-9075-9d8026341570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_prompts():\n",
    "    \n",
    "    # orgnize classes in chunck of 6 units\n",
    "    chunk_size = 6\n",
    "    chunked_class_names = [CLASS_NAMES_12[i:i + chunk_size] for i in range(0, len(CLASS_NAMES_12), chunk_size)]\n",
    "\n",
    "    dict_list = []  # list containing the generated dictionaries\n",
    "    for chunk in tqdm(chunked_class_names, desc=\"Generating dictionaries\"):\n",
    "        variants_dict = generate_classes_prompt_variants(chunk)   # generate the prompts for the chunk's classes\n",
    "        dict_list.append(variants_dict)\n",
    "\n",
    "    # Unify all the dictionries of the cunks in only one dictionary\n",
    "    unified_dict = defaultdict(list)\n",
    "    for diz in dict_list:\n",
    "        for key, val in diz.items():\n",
    "            unified_dict[key].extend(val)\n",
    "            \n",
    "    prompt_of_variants = dict(unified_dict)\n",
    "        \n",
    "    with open('generated_prompts.json', 'w') as f:\n",
    "        json.dump(prompt_of_variants, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03c965c8-c09c-431b-9fcb-9e763917123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dictionaries:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished class pink primrose - 1/6\n",
      "✅ Finished class hard-leaved pocket orchid - 2/6\n",
      "✅ Finished class canterbury bells - 3/6\n",
      "✅ Finished class sweet pea - 4/6\n",
      "✅ Finished class english marigold - 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dictionaries:  50%|█████     | 1/2 [00:37<00:37, 37.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished class tiger lily - 6/6\n",
      "✅ Finished class moon orchid - 1/6\n",
      "✅ Finished class bird of paradise - 2/6\n",
      "✅ Finished class monkshood - 3/6\n",
      "✅ Finished class globe thistle - 4/6\n",
      "✅ Finished class snapdragon - 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dictionaries: 100%|██████████| 2/2 [01:18<00:00, 39.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished class colt's foot - 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# init_prompts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba8d8828-f064-40ff-9612-edf9a7f1b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts(json_path='generated_prompts.json'):\n",
    "    \"\"\"\n",
    "    Reads the generated prompts from a JSON file and returns them as a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing the prompts.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with class names as keys and list of prompt variants as values.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        prompt_dict = json.load(f)\n",
    "    return prompt_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48741226-b82c-4775-927b-b0f1da8a972c",
   "metadata": {},
   "source": [
    "# TESTING\n",
    "1) Vanilla CLIP Zero Shot Classification\n",
    "2) Only Prmpts Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ea121b2-9b31-40e5-83ae-7c229072e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    # we know that the first 50 classes belong to the Base group, whilt the others to Novel\n",
    "    base_set = set(range(0, 51))\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    # here we create the dataset subsets; the torch Subset is just a wrapper around the dataset\n",
    "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
    "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    \n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3997bb75-57bd-49d6-8761-3b76771f314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision.datasets import Flowers102\n",
    "\n",
    "def get_test_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Load Oxford Flowers test split and class names\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e76b17-445c-423b-ab2e-a8620f639a54",
   "metadata": {},
   "source": [
    "## VANILLA CLIP ZERO SHOT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef29aa27-03df-454d-8665-5185db2c7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # we don't want gradients\n",
    "def eval_vanilla_clip(model, dataset, categories, batch_size, device, label=\"\"):\n",
    "    # let's set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
    "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
    "    text_inputs = clip.tokenize(\n",
    "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
    "    ).to(device)\n",
    "\n",
    "    # we can encode the text features once as they are shared for all images\n",
    "    # therefore we do it outside the evaluation loop\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    # and here we normalize them (standard pratice with CLIP)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward image through CLIP image encoder\n",
    "        image_features = model.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "294adbab-d119-4759-8bf7-cf3e9fcdec51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n",
      "[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 20/20 [00:12<00:00,  1.54it/s]\n",
      "🧠 Zero-shot evaluation on Novel Classes: 100%|██████████| 29/29 [00:18<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Base classes accuracy: 71.33%\n",
      "🔍 Novel classes accuracy: 78.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import Flowers102\n",
    "from torchvision import transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess_clip = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "test_dataset = get_test_data(transform=preprocess_clip)\n",
    "base_test_dataset, novel_test_dataset = split_data(test_dataset)\n",
    "\n",
    "base_classes = list(range(51))           # [0, 1, ..., 50]\n",
    "novel_classes = list(range(51, 102))     # [51, 52, ..., 101]\n",
    "\n",
    "base_accuracy = eval_vanilla_clip(model=model, dataset=base_test_dataset, categories=base_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Base Classes\")\n",
    "novel_accuracy = eval_vanilla_clip(model=model, dataset=novel_test_dataset, categories=novel_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "print()\n",
    "print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3d7f6-d0c9-46b1-be10-2f54eaf1281a",
   "metadata": {},
   "source": [
    "## ONLY PRMPTS AGUMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02f8da4f-64ea-4a31-a4d4-8eb38964d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad() # we don't want gradients\n",
    "def eval_only_prompts(model, dataset, prompt_dict, batch_size, device, label=\"\"):\n",
    "\n",
    "    model.eval()        # let's set the model in evaluation mode\n",
    "\n",
    "    # get features (average embedding for each class)\n",
    "    class_features = []\n",
    "\n",
    "    # Ordine fisso → ci servirà per mappare i target\n",
    "    ordered_keys = list(prompt_dict.keys())\n",
    "\n",
    "    for k in ordered_keys:\n",
    "        prompts = prompt_dict[k]                     # lista di stringhe\n",
    "\n",
    "        embeddings = []\n",
    "        for prompt in prompts:\n",
    "            tokens = clip.tokenize(prompt, truncate=True).to(device)  # [N_prompts, 77]\n",
    "\n",
    "            # Encode & normalize i prompt\n",
    "            emb = model.encode_text(tokens)        # [N_prompts, 512]\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append(emb.squeeze(0))  # [512]\n",
    "\n",
    "        # Media + rinormalizzazione\n",
    "        cls_emb = torch.stack(embeddings).mean(dim=0)                   # [512]\n",
    "        cls_emb = cls_emb / cls_emb.norm()\n",
    "\n",
    "        class_features.append(cls_emb)\n",
    "\n",
    "    # Stack → [num_classi, 512]\n",
    "    text_features = torch.stack(class_features)\n",
    "\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {idx: idx for idx, cat in enumerate(ordered_keys)}\n",
    "\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward image through CLIP image encoder\n",
    "        image_features = model.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a335ee8c-439e-44aa-9703-d11ad7249cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 20/20 [00:13<00:00,  1.47it/s]\n",
      "🧠 Zero-shot evaluation on Novel Classes: 100%|██████████| 29/29 [00:19<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Base classes accuracy: 66.96%\n",
      "🔍 Novel classes accuracy: 70.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import Flowers102\n",
    "from torchvision import transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess_clip = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "test_dataset = get_test_data(transform=preprocess_clip)\n",
    "base_test_dataset, novel_test_dataset = split_data(test_dataset)\n",
    "\n",
    "generated_prompts = load_prompts()\n",
    "\n",
    "base_accuracy = eval_only_prompts(model=model, dataset=base_test_dataset, prompt_dict=generated_prompts, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Base Classes\")\n",
    "novel_accuracy = eval_only_prompts(model=model, dataset=novel_test_dataset, prompt_dict=generated_prompts, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "print()\n",
    "print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e852e-d27c-4685-9d71-612c7ebb940a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
