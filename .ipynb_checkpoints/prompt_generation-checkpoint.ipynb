{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60897f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prompt generation\n",
    "\n",
    "This is a temporary notebook in which we will generate the prompt for clip given the set of classes from Flowers102."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e532365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# ! pip install -q -U google-genai\n",
    "# ! pip install tqdm\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "234bc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEMINI API KEY IN SECRET's SECTION (left menu)\n",
    "''' \n",
    "# If noteboock in Google Colab\n",
    "from google.colab import userdata\n",
    "userdata.get('GEMINI_API_KEY')\n",
    "'''\n",
    "\n",
    "# If noteboock in Jupiter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "# print(gemini_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3730176",
   "metadata": {},
   "source": [
    "**LIST of FLOWERS**\n",
    "First we define the label name of all the 102 flower classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f2f6a4f-f12b-4401-a75c-8d4be1648a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \n",
    "               \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \n",
    "               \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \n",
    "               \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \n",
    "               \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \n",
    "               \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \n",
    "               \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \n",
    "               \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \n",
    "               \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \n",
    "               \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \n",
    "               \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \n",
    "               \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \n",
    "               \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \n",
    "               \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \n",
    "               \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \n",
    "               \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \n",
    "               \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\",\n",
    "               \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \n",
    "               \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \n",
    "               \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \n",
    "               \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \n",
    "               \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "CLASS_NAMES_12 = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \n",
    "               \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \n",
    "               \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52656ec6",
   "metadata": {},
   "source": [
    "### Prompt Generation\n",
    "Here we ask throught API to Google Gemini to derive N prompts for each class. We define a clear prompt for Gemini where we define the number of prompts for each class and define a structured form of the output to better menage it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "928f4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_VARIANTS = 10\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are a vision-language expert.\n",
    "\n",
    "Input format\n",
    "• The user will give one Oxford-102 flower class name (e.g. “Daffodil”).\n",
    "\n",
    "Output format\n",
    "• Return **exactly {NUM_OF_VARIANTS}** short, visually grounded prompts.\n",
    "• Separate the prompts with **“; ”** (semicolon + space).\n",
    "• Do **not** add a semicolon after the final prompt.\n",
    "• Output nothing else.\n",
    "\n",
    "Content rules for each prompt\n",
    "1. Start with a different prefix chosen from this set (use each at most once):\n",
    "   “A photo of a”, “An image of a”, “A close-up of a”, “A macro shot of a”,\n",
    "   “A studio photo of a”, “An outdoor photo of a”, “A botanical plate of a”,\n",
    "   “A well-lit photograph of a”, “A side view of a”, “A top-down view of a”.\n",
    "2. Immediately include the **number of flowers** in the frame (e.g. “single”, “one”, “pair of”, “three”).\n",
    "3. Name the flower class exactly as given.\n",
    "4. Mention dominant colour(s) plus 1-2 key visual cues (petal count/shape, centre, leaves, stem).\n",
    "5. Keep each prompt ≤ 25 words.\n",
    "6. Use only information visible in a typical image; avoid subjective or non-visual terms (beautiful, symbolic, fragrant, etc.).\n",
    "7. End each prompt **without** a period.\n",
    "\n",
    "Return the ten prompts in one line, separated by “; ”, and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0736947d-0f05-4c96-a7fb-368d3ec0b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "525258ab-7d6f-4bad-8c7f-8f1e6cd169b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classes_prompt_variants(classes: list):\n",
    "    '''\n",
    "    This function generate a set of N prompts for each class in the input list parameter\n",
    "    '''\n",
    "    prompt_variants_dict = {}  \n",
    "\n",
    "    # Iterate each class: i is the class number, c is the class label\n",
    "    for i, c in enumerate(classes):\n",
    "        try:\n",
    "            # Send the request to gemini with API\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                config=types.GenerateContentConfig(\n",
    "                    system_instruction=SYSTEM_PROMPT,\n",
    "                    temperature=0.1\n",
    "                ),\n",
    "                contents=c\n",
    "            )\n",
    "            # Extract the response\n",
    "            all_prompts = response.text\n",
    "            prompt_variants_dict[c] = all_prompts.split('; ')\n",
    "            print(f\"✅ Finished class {c} - {i+1}/{len(classes)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating class '{c}' - {i+1}/{len(classes)}: {e}\")\n",
    "            prompt_variants_dict[c] = []  # oppure `None`, a seconda di cosa preferisci\n",
    "\n",
    "    return prompt_variants_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3bd47-dbc2-47f1-9ac5-ad3d5bdf2113",
   "metadata": {},
   "source": [
    "We initialize our system by define the prompts for each of the 102 classes of our database. Either this the code is orgnaized in order to flexible and easy to generate new prompts for new classes.\n",
    "\n",
    "To avoid overload Gemnin and risk to get lower quality result we devide the classes in small batches of 6 classes each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9004ba11-e4d4-48bb-9075-9d8026341570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_prompts():\n",
    "    \n",
    "    # orgnize classes in chunck of 6 units\n",
    "    chunk_size = 6\n",
    "    chunked_class_names = [CLASS_NAMES_12[i:i + chunk_size] for i in range(0, len(CLASS_NAMES_12), chunk_size)]\n",
    "\n",
    "    dict_list = []  # list containing the generated dictionaries\n",
    "    for chunk in tqdm(chunked_class_names, desc=\"Generating dictionaries\"):\n",
    "        variants_dict = generate_classes_prompt_variants(chunk)   # generate the prompts for the chunk's classes\n",
    "        dict_list.append(variants_dict)\n",
    "\n",
    "    # Unify all the dictionries of the cunks in only one dictionary\n",
    "    unified_dict = defaultdict(list)\n",
    "    for diz in dict_list:\n",
    "        for key, val in diz.items():\n",
    "            unified_dict[key].extend(val)\n",
    "            \n",
    "    prompt_of_variants = dict(unified_dict)\n",
    "        \n",
    "    with open('generated_prompts.json', 'w') as f:\n",
    "        json.dump(prompt_of_variants, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03c965c8-c09c-431b-9fcb-9e763917123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dictionaries:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished class pink primrose - 1/6\n",
      "✅ Finished class hard-leaved pocket orchid - 2/6\n",
      "✅ Finished class canterbury bells - 3/6\n",
      "✅ Finished class sweet pea - 4/6\n",
      "✅ Finished class english marigold - 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dictionaries:  50%|█████     | 1/2 [00:37<00:37, 37.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished class tiger lily - 6/6\n",
      "✅ Finished class moon orchid - 1/6\n",
      "✅ Finished class bird of paradise - 2/6\n",
      "✅ Finished class monkshood - 3/6\n",
      "✅ Finished class globe thistle - 4/6\n",
      "✅ Finished class snapdragon - 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dictionaries: 100%|██████████| 2/2 [01:18<00:00, 39.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished class colt's foot - 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# init_prompts()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48741226-b82c-4775-927b-b0f1da8a972c",
   "metadata": {},
   "source": [
    "# TESTING\n",
    "1) Vanilla CLIP Zero Shot Classification\n",
    "2) Our Increased Prmpts Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ea121b2-9b31-40e5-83ae-7c229072e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    # we know that the first 50 classes belong to the Base group, whilt the others to Novel\n",
    "    base_set = set(range(0, 51))\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    # here we create the dataset subsets; the torch Subset is just a wrapper around the dataset\n",
    "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
    "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    \n",
    "    return base_dataset, novel_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3997bb75-57bd-49d6-8761-3b76771f314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Flowers102\n",
    "\n",
    "# Load Oxford Flowers test split and class names\n",
    "flowers = Flowers102(root=\"./data\", split=\"test\", download=True)\n",
    "\n",
    "base_test_dataset, novel_test_dataset = split_data(flowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e76b17-445c-423b-ab2e-a8620f639a54",
   "metadata": {},
   "source": [
    "## VANILLA CLIP ZERO SHOT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef29aa27-03df-454d-8665-5185db2c7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # we don't want gradients\n",
    "def eval(model, dataset, batch_size, device, label=\"\"):\n",
    "   \n",
    "    model.eval()    # let's set the model in evaluation mode\n",
    "\n",
    "    prompts = [f\"a photo of a {name}\" for name in CLASS_NAMES]  # Standard prompt definition\n",
    "    text_tokens = clip.tokenize(prompts).to(device)             # Load the prompt to the device\n",
    "\n",
    "    text_features = model.encode_text(text_tokens)              # Encode the prompts\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)   # Normalize the encoding\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        target = torch.Tensor([t.item() for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward image through CLIP image encoder\n",
    "        image_features = model.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "base_accuracy = eval(model=model, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Base Classes\")\n",
    "novel_accuracy = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "print()\n",
    "print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294adbab-d119-4759-8bf7-cf3e9fcdec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from torchvision.datasets import Flowers102\n",
    "from torchvision import transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Encode prompts\n",
    "prompts = [f\"a photo of a {name}\" for name in CLASS_NAMES]  # Standard prompt definition\n",
    "text_tokens = clip.tokenize(prompts).to(device)             # Load the prompt to the device\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)              # Encode the prompts\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)   # Normalize the encoding\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
