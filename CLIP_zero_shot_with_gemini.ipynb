{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYQiprE0pcVD"
      },
      "source": [
        "# CLIP zero-shot Evaluation\n",
        "This short notebook implements the dataset split into base and novel categories (see project assignment) and runs the zero-shot evaluation with CLIP.\n",
        "Feel free to copy the code contained in this notebook or to directly use this notebook as starting point for you project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UzXtFjhh7iOS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Dependencies Installation\n",
        "%pip install -q google-genai pydantic openai_clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QtqdSOr8qqOn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torchvision\n",
        "import clip\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import gc\n",
        "from torchvision import transforms\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "from torchvision.datasets import Flowers102 as datset_used\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M_1CrUhZpVCq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Clip Utils functions\n",
        "\n",
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\n",
        "    Args:\n",
        "        data_dir (str): Directory where the dataset will be stored.\n",
        "        transform (torch.Compose)\n",
        "    Returns:\n",
        "        tuple: A tuple containing the train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train = datset_used(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = datset_used(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = datset_used(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    return train, val, test\n",
        "\n",
        "def base_novel_categories(dataset):\n",
        "    # set returns the unique set of all dataset classes\n",
        "    all_classes = set(dataset._labels)\n",
        "    # and let's count them\n",
        "    num_classes = len(all_classes)\n",
        "\n",
        "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
        "    # then we slice the list in half and generate base and novel category lists\n",
        "    base_classes = list(range(num_classes))[:num_classes//2]\n",
        "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
        "    return base_classes, novel_classes\n",
        "\n",
        "def split_data(dataset, base_classes):\n",
        "    # these two lists will store the sample indexes\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "\n",
        "    # we create a set of base classes to compute the test below in O(1)\n",
        "    # this is optional and can be removed\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    # here we iterate over sample labels and also get the correspondent sample index\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    # here we create the dataset subsets\n",
        "    # the torch Subset is just a wrapper around the dataset\n",
        "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
        "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
        "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "    return base_dataset, novel_dataset\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(model, dataset, categories, batch_size, device, text_features, label=\"\"):\n",
        "    from tqdm import tqdm\n",
        "    import numpy as np\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Remap categories to contiguous label space\n",
        "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    correct_top1 = 0\n",
        "    correct_top5 = 0\n",
        "    correct_top10 = 0\n",
        "    total = 0\n",
        "\n",
        "    # Confidence gaps split by error category\n",
        "    gap_top1_hit = []\n",
        "    gap_top5_hit = []\n",
        "    gap_top10_hit = []\n",
        "    gap_top10_miss = []\n",
        "\n",
        "    for images, targets in tqdm(dataloader, desc=label):\n",
        "        targets = torch.tensor([contig_cat2idx[t.item()] for t in targets], dtype=torch.long).to(device)\n",
        "        images = images.to(device)\n",
        "\n",
        "        image_features = model.encode_image(images)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarities = image_features @ text_features.T\n",
        "        top10 = similarities.topk(10, dim=-1)\n",
        "\n",
        "        top10_indices = top10.indices\n",
        "        top10_values = top10.values\n",
        "\n",
        "        # Accuracies\n",
        "        correct_top1 += (top10_indices[:, 0] == targets).sum().item()\n",
        "        correct_top5 += sum([targets[i] in top10_indices[i, :5] for i in range(len(targets))])\n",
        "        correct_top10 += sum([targets[i] in top10_indices[i, :10] for i in range(len(targets))])\n",
        "\n",
        "        for i in range(len(targets)):\n",
        "            true_idx = targets[i].item()\n",
        "            pred_conf = top10_values[i, 0].item()\n",
        "            true_conf = similarities[i, true_idx].item()\n",
        "\n",
        "            if top10_indices[i, 0].item() == true_idx:\n",
        "                second_conf = top10_values[i, 1].item()\n",
        "                gap_top1_hit.append((pred_conf - second_conf) * 100)\n",
        "                continue\n",
        "            if true_idx in top10_indices[i, 1:5]:\n",
        "                gap_top5_hit.append((pred_conf - true_conf) * 100)\n",
        "            elif true_idx in top10_indices[i, 5:10]:\n",
        "                gap_top10_hit.append((pred_conf - true_conf) * 100)\n",
        "            else:\n",
        "                gap_top10_miss.append((pred_conf - true_conf) * 100)\n",
        "\n",
        "        total += targets.size(0)\n",
        "\n",
        "    # Final metrics\n",
        "    top1_acc = correct_top1 / total\n",
        "    top5_acc = correct_top5 / total\n",
        "    top10_acc = correct_top10 / total\n",
        "\n",
        "    print(f\"\\nüìä Total samples evaluated: {total}\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Top-1 Accuracy:      {top1_acc*100:.2f}%\")\n",
        "    print(f\"‚úÖ Top-5 Accuracy:      {top5_acc*100:.2f}%\")\n",
        "    print(f\"‚úÖ Top-10 Accuracy:     {top10_acc*100:.2f}%\")\n",
        "    def safe_mean(arr): return np.mean(arr) if arr else 0.0\n",
        "    print(f\"‚úÖ Avg. Conf. Gap (Top-1 hit):      {safe_mean(gap_top1_hit):.2f}%\")\n",
        "\n",
        "    print(f\"‚ùå Avg. Conf. Gap (Top-5 hit):     {safe_mean(gap_top5_hit):.2f}%\")\n",
        "    print(f\"‚ùå Avg. Conf. Gap (Top-10 hit):    {safe_mean(gap_top10_hit):.2f}%\")\n",
        "    print(f\"‚ùå Avg. Conf. Gap (Beyond top-10): {safe_mean(gap_top10_miss):.2f}%\")\n",
        "\n",
        "    del text_features, dataloader, image_features, similarities, targets, images, top10, top10_indices, top10_values\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "\n",
        "    return {\n",
        "    \"top1\": top1_acc,\n",
        "    \"top5\": top5_acc,\n",
        "    \"top10\": top10_acc,\n",
        "    \"avg_gap_top1_hit\": safe_mean(gap_top1_hit),\n",
        "    \"avg_error_top5_hit\": safe_mean(gap_top5_hit),\n",
        "    \"avg_error_top10_hit\": safe_mean(gap_top10_hit),\n",
        "    \"avg_error_top10_miss\": safe_mean(gap_top10_miss),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "Sh6uLZRT7YJx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title CLIP Loading\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "\n",
        "# get the three datasets\n",
        "train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "\n",
        "# split classes into base and novel\n",
        "base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "# split the three datasets\n",
        "train_base, _ = split_data(train_set, base_classes)\n",
        "val_base, _ = split_data(val_set, base_classes)\n",
        "test_base, test_novel = split_data(test_set, base_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uhblkvm9US4",
        "outputId": "a9b2ecd6-3cff-4896-acab-cb4572ed88c5",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Base Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:19<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Total samples evaluated: 2473\n",
            "\n",
            "‚úÖ Top-1 Accuracy:      71.29%\n",
            "‚úÖ Top-5 Accuracy:      90.86%\n",
            "‚úÖ Top-10 Accuracy:     97.53%\n",
            "‚ùå Avg. Conf. Gap (Top-5 hit):     1.67%\n",
            "‚ùå Avg. Conf. Gap (Top-10 hit):    3.86%\n",
            "‚ùå Avg. Conf. Gap (Beyond top-10): 5.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:29<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Total samples evaluated: 3676\n",
            "\n",
            "‚úÖ Top-1 Accuracy:      78.24%\n",
            "‚úÖ Top-5 Accuracy:      89.15%\n",
            "‚úÖ Top-10 Accuracy:     92.79%\n",
            "‚ùå Avg. Conf. Gap (Top-5 hit):     1.37%\n",
            "‚ùå Avg. Conf. Gap (Top-10 hit):    3.45%\n",
            "‚ùå Avg. Conf. Gap (Beyond top-10): 5.70%\n"
          ]
        }
      ],
      "source": [
        "#@title Compute Zero-Shot Standard Predictions\n",
        "\n",
        "base_text_inputs = clip.tokenize(\n",
        "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in base_classes]\n",
        "    ).to(device)\n",
        "base_text_features = model.encode_text(base_text_inputs)\n",
        "base_text_features /= base_text_features.norm(dim=-1, keepdim=True)\n",
        "base_accuracy = eval(model=model, dataset=test_base, categories=base_classes, batch_size=128, device=device, text_features=base_text_features, label=\"üß† Zero-shot evaluation on Base Classes\")\n",
        "del base_text_inputs\n",
        "\n",
        "novel_text_inputs = clip.tokenize(\n",
        "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in novel_classes]\n",
        "    ).to(device)\n",
        "novel_text_features = model.encode_text(novel_text_inputs)\n",
        "novel_text_features /= novel_text_features.norm(dim=-1, keepdim=True)\n",
        "novel_accuracies = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, text_features=novel_text_features, label=\"üß† Zero-shot evaluation on Novel Classes\")\n",
        "del novel_text_inputs\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zTnF4b47HMA",
        "outputId": "0b5ee181-83d3-410d-988d-dd8905e37da4",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENAI_KEY not found in Colab secrets. Please add it to proceed.\n",
            "Google GenAI SDK configured successfully!\n",
            "Pydantic schema defined.\n"
          ]
        }
      ],
      "source": [
        "#@title Gemini setup\n",
        "# Load your API key from Colab secrets.\n",
        "try:\n",
        "    GENAI_API_KEY = userdata.get('GENAI_KEY')\n",
        "    genai.configure(api_key=GENAI_API_KEY)\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"GENAI_KEY not found in Colab secrets. Please add it to proceed.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during API key setup: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(\"Google GenAI SDK configured successfully!\")\n",
        "\n",
        "# Define the Pydantic schema for the LLM output\n",
        "class PromptDescriptions(BaseModel):\n",
        "    # This will hold a list of lists: [[desc1_flower1, desc2_flower1, ...], [desc1_flower2, ...], ...]\n",
        "    flower_descriptions: List[List[str]] = Field(\n",
        "        description=\"A list where each element is a list of 5 short descriptions for a specific flower.\"\n",
        "    )\n",
        "\n",
        "print(\"Pydantic schema defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqPWs3Wk8tM_",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 3. LLM Prompt Generation (Batched Calls)\n",
        "\n",
        "genai_model = genai.GenerativeModel('gemini-2.5-flash')  # Using Flash for speed\n",
        "\n",
        "generated_prompts_for_classes = {}  # {class_name: [list of 5 prompts]}\n",
        "prompt_batch_size = 13\n",
        "\n",
        "# Define schema using Pydantic\n",
        "class Prompts(BaseModel):\n",
        "    prompt1: str\n",
        "    prompt2: str\n",
        "    prompt3: str\n",
        "    prompt4: str\n",
        "    prompt5: str\n",
        "\n",
        "class PromptDescriptions(BaseModel):\n",
        "    flower_descriptions: List[Prompts]\n",
        "\n",
        "print(\"\\n--- Generating 5 prompts for each flower class using LLM ---\")\n",
        "print(\"This might take a few minutes depending on the number of classes and API response times.\")\n",
        "\n",
        "for i in tqdm(range(0, len(CLASS_NAMES), prompt_batch_size), desc=\"Generating prompts in batches\"):\n",
        "    current_batch_classes = CLASS_NAMES[i : min(i + prompt_batch_size, len(CLASS_NAMES))]\n",
        "\n",
        "    # Dynamically build the prompt for the current batch\n",
        "    prompt_batch = f\"\"\"You are a professional botanical photographer and creative writer for a visual nature magazine.\n",
        "\n",
        "Given a list of flower species, generate exactly 5 short and visually evocative descriptions per flower. Each description should be phrased as if accompanying a stunning photograph of the flower, aiming to evoke its unique beauty and essence for an AI model like CLIP.\n",
        "\n",
        "Each prompt must be:\n",
        "- Descriptive, with strong visual language (color, shape, textures, dimension of petals, feeling)\n",
        "- Distinct across the 5 prompts (no overlap or simple rewordings)\n",
        "- Specific to the given flower (no generic phrases)\n",
        "- Emphasize features that distinguish this flower from visually similar species\n",
        "- Realistic and natural (imagine you're writing the description of your own photo of the flower)\n",
        "\n",
        "Use the word 'flower' in at least 2 out of 5 prompts. Prioritize conciseness (max 20 words) and features that distinguish the flower.\n",
        "\n",
        "Return the output as a JSON object with a single key 'flower_descriptions', whose value is a list of objects. Each object corresponds to a flower and includes exactly 5 fields: 'prompt1' through 'prompt5' (all strings).\n",
        "\n",
        "Here are the flowers for this batch: {current_batch_classes} \"\"\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        response_llm = genai_model.generate_content(\n",
        "            prompt_batch,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                response_mime_type=\"application/json\",\n",
        "                response_schema=PromptDescriptions\n",
        "            )\n",
        "        )\n",
        "\n",
        "        parsed_response = PromptDescriptions.model_validate_json(response_llm.text)\n",
        "\n",
        "        for j, prompt_obj in enumerate(parsed_response.flower_descriptions):\n",
        "            flower_name = current_batch_classes[j]\n",
        "            generated_prompts_for_classes[flower_name] = [\n",
        "                prompt_obj.prompt1,\n",
        "                prompt_obj.prompt2,\n",
        "                prompt_obj.prompt3,\n",
        "                prompt_obj.prompt4,\n",
        "                prompt_obj.prompt5,\n",
        "            ]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError generating prompts for batch starting with '{current_batch_classes[0]}': {e}\")\n",
        "        print(f\"Raw response (if available): {response_llm.text if 'response_llm' in locals() else 'N/A'}\")\n",
        "\n",
        "print(\"\\nLLM prompt generation complete.\")\n",
        "\n",
        "with open(\"generated_prompts.json\", \"w\") as f:\n",
        "    json.dump(generated_prompts_for_classes, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import generated Prompts\n",
        "import json\n",
        "\n",
        "# 1. Load prompts\n",
        "with open(\"generated_prompts.json\", \"r\") as f:\n",
        "    generated_prompts_for_classes = json.load(f)\n",
        "\n",
        "# 2. Generate averaged text embeddings per class\n",
        "def get_llm_text_features(model, prompt_dict, class_ids, class_names, device):\n",
        "    \"\"\"\n",
        "    Compute mean text embedding per class from LLM-generated prompts.\n",
        "    \"\"\"\n",
        "    text_features = []\n",
        "\n",
        "    for c in class_ids:\n",
        "        class_name = class_names[c]\n",
        "        prompts = prompt_dict[class_name]\n",
        "\n",
        "        # Tokenize and encode prompts\n",
        "        text_inputs = clip.tokenize(prompts).to(device)\n",
        "        embeddings = model.encode_text(text_inputs)\n",
        "        embeddings /= embeddings.norm(dim=-1, keepdim=True)  # Normalize\n",
        "\n",
        "        mean_embedding = embeddings.mean(dim=0)\n",
        "        mean_embedding /= mean_embedding.norm(dim=-1, keepdim=True)  # Re-normalize after mean\n",
        "        text_features.append(mean_embedding)\n",
        "        del text_inputs, embeddings, mean_embedding\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return torch.stack(text_features).to(device)\n"
      ],
      "metadata": {
        "id": "jIsJyEaaOdHC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Base classes evaluation\n",
        "base_text_features = get_llm_text_features(\n",
        "    model=model,\n",
        "    prompt_dict=generated_prompts_for_classes,\n",
        "    class_ids=base_classes,\n",
        "    class_names=CLASS_NAMES,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Evaluate using precomputed text features\n",
        "base_accuracies = eval(\n",
        "    model=model,\n",
        "    dataset=test_base,\n",
        "    categories=base_classes,\n",
        "    batch_size=128,\n",
        "    device=device,\n",
        "    text_features=base_text_features,\n",
        "    label=\"üå∏ Zero-shot eval with LLM prompts on Base Classes\"\n",
        ")\n",
        "\n",
        "# Clean up large tensors\n",
        "del base_text_features, base_accuracies\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOO9y6fKkz2H",
        "outputId": "024df5fe-0e66-4787-df7a-8ab9b7ba8031"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üå∏ Zero-shot eval with LLM prompts on Base Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:19<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Total samples evaluated: 2473\n",
            "\n",
            "‚úÖ Top-1 Accuracy:      79.18%\n",
            "‚úÖ Top-5 Accuracy:      94.99%\n",
            "‚úÖ Top-10 Accuracy:     96.24%\n",
            "‚úÖ Avg. Conf. Gap (Top-1 hit):      3.85%\n",
            "‚ùå Avg. Conf. Gap (Top-5 hit):     1.51%\n",
            "‚ùå Avg. Conf. Gap (Top-10 hit):    4.15%\n",
            "‚ùå Avg. Conf. Gap (Beyond top-10): 7.13%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Novel classes evaluation\n",
        "novel_text_features = get_llm_text_features(\n",
        "    model=model,\n",
        "    prompt_dict=generated_prompts_for_classes,\n",
        "    class_ids=novel_classes,\n",
        "    class_names=CLASS_NAMES,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Evaluate using precomputed text features\n",
        "novel_accuracies = eval(\n",
        "    model=model,\n",
        "    dataset=test_novel,\n",
        "    categories=novel_classes,\n",
        "    batch_size=128,\n",
        "    device=device,\n",
        "    text_features=novel_text_features,\n",
        "    label=\"üå∏ Zero-shot eval with LLM prompts on Novel Classes\"\n",
        ")\n",
        "\n",
        "# Clean up large tensors\n",
        "del novel_text_features, novel_accuracies\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOmMp9v2kxHL",
        "outputId": "e9938ed8-b36b-40e3-8ad1-0141bd38cc6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üå∏ Zero-shot eval with LLM prompts on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:28<00:00,  1.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Total samples evaluated: 3676\n",
            "\n",
            "‚úÖ Top-1 Accuracy:      80.66%\n",
            "‚úÖ Top-5 Accuracy:      92.19%\n",
            "‚úÖ Top-10 Accuracy:     93.25%\n",
            "‚úÖ Avg. Conf. Gap (Top-1 hit):      4.32%\n",
            "‚ùå Avg. Conf. Gap (Top-5 hit):     1.93%\n",
            "‚ùå Avg. Conf. Gap (Top-10 hit):    4.39%\n",
            "‚ùå Avg. Conf. Gap (Beyond top-10): 13.61%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baYfLKNdfbUR"
      },
      "source": [
        "## Harmonic Mean\n",
        "Few-Shot Adaptations papers usually report the Harmonic Mean.\n",
        "The harmonic mean tends to mitigate the impact of large outliers (base accuracy) and aggravate the impact of small ones (novel accuracy).\n",
        "Thus, achieving very high base accuracies at the expense of the novel accuracy will be penalized by the HM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKAXR7hlfbUR",
        "outputId": "22515c6e-0d83-46d2-ed7e-e72b19ac4a50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Harmonic Mean: 74.62%\n"
          ]
        }
      ],
      "source": [
        "def harmonic_mean(base_accuracy, novel_accuracy):\n",
        "    numerator = 2\n",
        "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
        "    hm = numerator / denominator\n",
        "    return hm\n",
        "\n",
        "print(f\"üîç Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}