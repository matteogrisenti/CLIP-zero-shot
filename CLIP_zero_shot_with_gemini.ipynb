{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYQiprE0pcVD"
      },
      "source": [
        "# CLIP zero-shot Evaluation\n",
        "This short notebook implements the dataset split into base and novel categories (see project assignment) and runs the zero-shot evaluation with CLIP.\n",
        "Feel free to copy the code contained in this notebook or to directly use this notebook as starting point for you project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UzXtFjhh7iOS",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b2cd75-af93-4b09-851f-8ae2dff0e477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.6/1.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai_clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#@title Dependencies Installation\n",
        "%pip install -q google-genai pydantic openai_clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QtqdSOr8qqOn",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b13e207-b166-43d2-af83-2855128f9cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All dependencies imported successfully!\n",
            "üî• PyTorch version: 2.6.0+cu124\n",
            "üñºÔ∏è Torchvision version: 0.21.0+cu124\n",
            "ü§ñ CUDA available: True\n",
            "üéÆ GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torchvision\n",
        "import clip\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import gc\n",
        "from torchvision import transforms\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "from torchvision.datasets import Flowers102 as dataset_used\n",
        "import contextlib\n",
        "import numpy as np\n",
        "\n",
        "print(\"‚úÖ All dependencies imported successfully!\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üñºÔ∏è Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"ü§ñ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Classes setup\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
        "\n",
        "print(f\"üìä Total flower classes: {len(CLASS_NAMES)}\")\n",
        "print(f\"üå∏ Sample classes: {CLASS_NAMES[:5]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "feUqkSTsvj3b",
        "outputId": "cd1a9654-6e1d-4e3b-d833-33ff3e997771"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Total flower classes: 102\n",
            "üå∏ Sample classes: ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Memory management utils\n",
        "def aggressive_cleanup():\n",
        "    \"\"\"\n",
        "    Comprehensive memory cleanup function to prevent VRAM leaks.\n",
        "\n",
        "    This function:\n",
        "    - Forces Python garbage collection\n",
        "    - Empties CUDA cache\n",
        "    - Collects IPC (Inter-Process Communication) resources\n",
        "    - Synchronizes CUDA operations\n",
        "    \"\"\"\n",
        "    gc.collect()                    # Python garbage collection\n",
        "    torch.cuda.empty_cache()        # Clear CUDA cache\n",
        "    torch.cuda.ipc_collect()        # Clean up IPC resources\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()    # Wait for all CUDA operations to complete\n",
        "\n",
        "def print_memory_stats():\n",
        "    \"\"\"Display current GPU memory usage statistics\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3    # Convert to GB\n",
        "        print(f\"üîã GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
        "    else:\n",
        "        print(\"‚ùå CUDA not available - running on CPU\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HvcFJ1LL7xO3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M_1CrUhZpVCq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Utils functions\n",
        "\n",
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\n",
        "    Args:\n",
        "        data_dir (str): Directory where the dataset will be stored.\n",
        "        transform (torch.Compose)\n",
        "    Returns:\n",
        "        tuple: A tuple containing the train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    print(\"üì• Downloading and loading Flowers102 dataset...\")\n",
        "    train = dataset_used(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = dataset_used(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = dataset_used(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    print(f\"‚úÖ Dataset loaded - Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
        "    return train, val, test\n",
        "\n",
        "def base_novel_categories(dataset):\n",
        "    \"\"\"\n",
        "    Split all classes into base and novel categories.\n",
        "\n",
        "    Base classes: First half of classes (for training/fine-tuning scenarios)\n",
        "    Novel classes: Second half of classes (for zero-shot evaluation)\n",
        "\n",
        "    Args:\n",
        "        dataset: PyTorch dataset object\n",
        "\n",
        "    Returns:\n",
        "        base_classes, novel_classes: Lists of class indices\n",
        "    \"\"\"\n",
        "    all_classes = set(dataset._labels)\n",
        "    num_classes = len(all_classes)\n",
        "\n",
        "    # Split classes 50/50\n",
        "    base_classes = list(range(num_classes))[:num_classes//2]\n",
        "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
        "\n",
        "    print(f\"üìä Class split - Base: {len(base_classes)}, Novel: {len(novel_classes)}\")\n",
        "    return base_classes, novel_classes\n",
        "\n",
        "def split_data(dataset, base_classes):\n",
        "    \"\"\"\n",
        "    Split dataset samples based on base/novel class membership.\n",
        "\n",
        "    Args:\n",
        "        dataset: PyTorch dataset\n",
        "        base_classes: List of base class indices\n",
        "\n",
        "    Returns:\n",
        "        base_dataset, novel_dataset: Subsets containing only base/novel samples\n",
        "    \"\"\"\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    # Iterate through all samples and categorize by class\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    # Create dataset subsets\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "\n",
        "    print(f\"üìä Data split - Base samples: {len(base_dataset)}, Novel samples: {len(novel_dataset)}\")\n",
        "    return base_dataset, novel_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CLIP context manager\n",
        "@contextlib.contextmanager\n",
        "def clip_model_context(model_name=\"ViT-B/16\"):\n",
        "    \"\"\"\n",
        "    Context manager for CLIP model to ensure proper cleanup.\n",
        "\n",
        "    This ensures that:\n",
        "    - Model is properly loaded on the correct device\n",
        "    - Model is set to evaluation mode\n",
        "    - Memory is cleaned up after use, even if errors occur\n",
        "\n",
        "    Args:\n",
        "        model_name: CLIP model variant to load\n",
        "\n",
        "    Yields:\n",
        "        model: CLIP model\n",
        "        preprocess: Image preprocessing function\n",
        "        device: Device (cuda/cpu) the model is loaded on\n",
        "    \"\"\"\n",
        "    print(f\"ü§ñ Loading CLIP model: {model_name}\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Load CLIP model and preprocessing\n",
        "    model, preprocess = clip.load(model_name, device=device)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    print(f\"‚úÖ CLIP model loaded on {device}\")\n",
        "\n",
        "    try:\n",
        "        yield model, preprocess, device\n",
        "    finally:\n",
        "        # Ensure cleanup happens even if errors occur\n",
        "        print(\"üßπ Cleaning up CLIP model...\")\n",
        "        del model\n",
        "        aggressive_cleanup()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QDBGk6Z68gbQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluation function\n",
        "@torch.no_grad()\n",
        "def eval(model, dataset, categories, batch_size, device, text_features, label=\"\"):\n",
        "    \"\"\"\n",
        "    Memory-optimized evaluation function for CLIP zero-shot classification.\n",
        "\n",
        "    Computes:\n",
        "    - Top-1, Top-5, Top-10 accuracies\n",
        "    - Confidence gaps between predicted and true classes\n",
        "    - Splits confidence gaps by error type (top-1 hit, top-5 hit, etc.)\n",
        "\n",
        "    Args:\n",
        "        model: CLIP model\n",
        "        dataset: PyTorch dataset to evaluate\n",
        "        categories: List of class indices for this evaluation\n",
        "        batch_size: Batch size for evaluation\n",
        "        device: Device to run on\n",
        "        text_features: Pre-computed text embeddings\n",
        "        label: Description label for progress bar\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with accuracy metrics and confidence gaps\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Create mapping from original class IDs to contiguous indices (0, 1, 2, ...)\n",
        "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "    # Create dataloader (num_workers=0 to avoid subprocess memory issues in Colab)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
        "    )\n",
        "\n",
        "    # Initialize counters\n",
        "    correct_top1 = 0\n",
        "    correct_top5 = 0\n",
        "    correct_top10 = 0\n",
        "    total = 0\n",
        "\n",
        "    # Lists to store confidence gaps for different error categories\n",
        "    gap_top1_hit = []      # When prediction is correct\n",
        "    gap_top5_hit = []      # When true class is in top-5 but not top-1\n",
        "    gap_top10_hit = []     # When true class is in top-10 but not top-5\n",
        "    gap_top10_miss = []    # When true class is not in top-10\n",
        "\n",
        "    try:\n",
        "        for batch_idx, (images, targets) in enumerate(tqdm(dataloader, desc=label)):\n",
        "            # Remap targets to contiguous space and move to device\n",
        "            targets = torch.tensor([contig_cat2idx[t.item()] for t in targets], dtype=torch.long).to(device)\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Encode images\n",
        "            image_features = model.encode_image(images)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Compute similarities between images and text\n",
        "            similarities = image_features @ text_features.T\n",
        "\n",
        "            # Get top-10 predictions\n",
        "            top10 = similarities.topk(10, dim=-1)\n",
        "            top10_indices = top10.indices  # Class predictions\n",
        "            top10_values = top10.values    # Confidence scores\n",
        "\n",
        "            # Calculate accuracies\n",
        "            correct_top1 += (top10_indices[:, 0] == targets).sum().item()\n",
        "            correct_top5 += sum([targets[i] in top10_indices[i, :5] for i in range(len(targets))])\n",
        "            correct_top10 += sum([targets[i] in top10_indices[i, :10] for i in range(len(targets))])\n",
        "\n",
        "            # Calculate confidence gaps for each sample\n",
        "            for i in range(len(targets)):\n",
        "                true_idx = targets[i].item()\n",
        "                pred_conf = top10_values[i, 0].item()  # Highest prediction confidence\n",
        "                true_conf = similarities[i, true_idx].item()  # True class confidence\n",
        "\n",
        "                # Categorize by error type\n",
        "                if top10_indices[i, 0].item() == true_idx:\n",
        "                    # Correct prediction - gap between 1st and 2nd choice\n",
        "                    second_conf = top10_values[i, 1].item()\n",
        "                    gap_top1_hit.append((pred_conf - second_conf) * 100)\n",
        "                elif true_idx in top10_indices[i, 1:5]:\n",
        "                    # True class in top-5 but not top-1\n",
        "                    gap_top5_hit.append((pred_conf - true_conf) * 100)\n",
        "                elif true_idx in top10_indices[i, 5:10]:\n",
        "                    # True class in top-10 but not top-5\n",
        "                    gap_top10_hit.append((pred_conf - true_conf) * 100)\n",
        "                else:\n",
        "                    # True class not in top-10\n",
        "                    gap_top10_miss.append((pred_conf - true_conf) * 100)\n",
        "\n",
        "            total += targets.size(0)\n",
        "\n",
        "            # Clean up batch tensors immediately to save memory\n",
        "            del images, targets, image_features, similarities, top10, top10_indices, top10_values\n",
        "\n",
        "            # Periodic cleanup during long evaluations\n",
        "            if batch_idx % 10 == 0:\n",
        "                aggressive_cleanup()\n",
        "\n",
        "    finally:\n",
        "        # Clean up dataloader\n",
        "        del dataloader\n",
        "        aggressive_cleanup()\n",
        "\n",
        "    # Calculate final metrics\n",
        "    top1_acc = correct_top1 / total\n",
        "    top5_acc = correct_top5 / total\n",
        "    top10_acc = correct_top10 / total\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nüìä Total samples evaluated: {total}\\n\")\n",
        "    print(f\"‚úÖ Top-1 Accuracy:      {top1_acc*100:.2f}%\")\n",
        "    print(f\"‚úÖ Top-5 Accuracy:      {top5_acc*100:.2f}%\")\n",
        "    print(f\"‚úÖ Top-10 Accuracy:     {top10_acc*100:.2f}%\")\n",
        "    print(f\"‚úÖ Avg. Conf. Gap (Top-1 hit):      {safe_mean(gap_top1_hit):.2f}%\")\n",
        "    print(f\"‚ùå Avg. Conf. Gap (Top-5 hit):     {safe_mean(gap_top5_hit):.2f}%\")\n",
        "    print(f\"‚ùå Avg. Conf. Gap (Top-10 hit):    {safe_mean(gap_top10_hit):.2f}%\")\n",
        "    print(f\"‚ùå Avg. Conf. Gap (Beyond top-10): {safe_mean(gap_top10_miss):.2f}%\")\n",
        "\n",
        "    return {\n",
        "        \"top1\": top1_acc,\n",
        "        \"top5\": top5_acc,\n",
        "        \"top10\": top10_acc,\n",
        "        \"avg_gap_top1_hit\": safe_mean(gap_top1_hit),\n",
        "        \"avg_error_top5_hit\": safe_mean(gap_top5_hit),\n",
        "        \"avg_error_top10_hit\": safe_mean(gap_top10_hit),\n",
        "        \"avg_error_top10_miss\": safe_mean(gap_top10_miss),\n",
        "    }"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mrRHjqc68W2v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Text features functions\n",
        "def get_text_features_standard(model, class_ids, device):\n",
        "    \"\"\"\n",
        "    Generate standard CLIP text features using simple template prompts.\n",
        "\n",
        "    Uses the template: \"a photo of a {class_name}, a type of flower.\"\n",
        "\n",
        "    Args:\n",
        "        model: CLIP model\n",
        "        class_ids: List of class indices to generate features for\n",
        "        device: Device to run computation on\n",
        "\n",
        "    Returns:\n",
        "        text_features: Normalized text embeddings tensor\n",
        "    \"\"\"\n",
        "    # Create simple template prompts\n",
        "    prompts = [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in class_ids]\n",
        "    print(f\"üìù Generated {len(prompts)} standard prompts\")\n",
        "    print(f\"üìÑ Example prompt: '{prompts[0]}'\")\n",
        "\n",
        "    # Tokenize prompts\n",
        "    text_inputs = clip.tokenize(prompts).to(device)\n",
        "\n",
        "    try:\n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "            text_features = model.encode_text(text_inputs)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Create a copy to ensure we don't keep references\n",
        "        result = text_features.clone()\n",
        "        return result\n",
        "\n",
        "    finally:\n",
        "        # Clean up intermediate tensors\n",
        "        del text_inputs, text_features\n",
        "        aggressive_cleanup()\n",
        "\n",
        "def get_llm_text_features(model, prompt_dict, class_ids, class_names, device):\n",
        "    \"\"\"\n",
        "    Generate text features from LLM-generated prompts.\n",
        "\n",
        "    For each class, uses multiple detailed prompts generated by an LLM,\n",
        "    then averages their embeddings to get a richer representation.\n",
        "\n",
        "    Args:\n",
        "        model: CLIP model\n",
        "        prompt_dict: Dictionary mapping class names to lists of prompts\n",
        "        class_ids: List of class indices\n",
        "        class_names: List of all class names\n",
        "        device: Device to run computation on\n",
        "\n",
        "    Returns:\n",
        "        text_features: Tensor of averaged normalized embeddings per class\n",
        "    \"\"\"\n",
        "    text_features = []\n",
        "\n",
        "    print(f\"ü§ñ Processing LLM-generated prompts for {len(class_ids)} classes...\")\n",
        "\n",
        "    for c in class_ids:\n",
        "        class_name = class_names[c]\n",
        "        prompts = prompt_dict[class_name]\n",
        "\n",
        "        print(f\"üìù Processing {len(prompts)} prompts for '{class_name}'\")\n",
        "\n",
        "        # Tokenize all prompts for this class\n",
        "        text_inputs = clip.tokenize(prompts).to(device)\n",
        "\n",
        "        try:\n",
        "            # Generate embeddings for all prompts\n",
        "            with torch.no_grad():\n",
        "                embeddings = model.encode_text(text_inputs)\n",
        "                embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                # Average across all prompts for this class\n",
        "                mean_embedding = embeddings.mean(dim=0)\n",
        "                mean_embedding = mean_embedding / mean_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                # Store the averaged embedding\n",
        "                text_features.append(mean_embedding.clone())\n",
        "\n",
        "        finally:\n",
        "            # Clean up intermediate tensors\n",
        "            del text_inputs, embeddings, mean_embedding\n",
        "            aggressive_cleanup()\n",
        "\n",
        "    return torch.stack(text_features).to(device)\n",
        "\n",
        "def safe_mean(arr):\n",
        "    \"\"\"Safely compute mean, returning 0 if array is empty\"\"\"\n",
        "    return np.mean(arr) if arr else 0.0"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vCVSIRgR8zzT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluation suite\n",
        "def run_evaluation_suite():\n",
        "    \"\"\"\n",
        "    Run complete evaluation suite with proper memory management.\n",
        "\n",
        "    Evaluations performed:\n",
        "    1. Standard prompts on base classes\n",
        "    2. Standard prompts on novel classes\n",
        "    3. LLM-enhanced prompts on base classes (if available)\n",
        "    4. LLM-enhanced prompts on novel classes (if available)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing all evaluation results\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Starting memory-optimized CLIP evaluation suite...\")\n",
        "\n",
        "    # Try to load LLM-generated prompts\n",
        "    try:\n",
        "        with open(\"generated_prompts.json\", \"r\") as f:\n",
        "            generated_prompts_for_classes = json.load(f)\n",
        "        has_llm_prompts = True\n",
        "        print(\"‚úÖ Found generated prompts file\")\n",
        "        print(f\"üìä Loaded prompts for {len(generated_prompts_for_classes)} classes\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå No generated prompts found. Will only run standard evaluation.\")\n",
        "        has_llm_prompts = False\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Use context manager for proper model lifecycle management\n",
        "    with clip_model_context(\"ViT-B/16\") as (model, preprocess, device):\n",
        "        print(f\"üì± Using device: {device}\")\n",
        "\n",
        "        # Load and prepare datasets\n",
        "        print(\"\\nüì• Loading and preparing datasets...\")\n",
        "        train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "        base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "        # Split datasets by base/novel classes\n",
        "        train_base, _ = split_data(train_set, base_classes)\n",
        "        val_base, _ = split_data(val_set, base_classes)\n",
        "        test_base, test_novel = split_data(test_set, base_classes)\n",
        "\n",
        "        print(f\"üìä Dataset prepared:\")\n",
        "        print(f\"   Base classes: {len(base_classes)} Novel classes: {len(novel_classes)}\")\n",
        "        print(f\"   Test base samples: {len(test_base)} Test novel samples: {len(test_novel)}\")\n",
        "\n",
        "        # EVALUATION 1: Standard Base Classes\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üîÑ EVALUATION 1: Standard Prompts on Base Classes\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        base_text_features = get_text_features_standard(model, base_classes, device)\n",
        "        try:\n",
        "            results['standard_base'] = eval(\n",
        "                model=model,\n",
        "                dataset=test_base,\n",
        "                categories=base_classes,\n",
        "                batch_size=64,  # Conservative batch size for Colab\n",
        "                device=device,\n",
        "                text_features=base_text_features,\n",
        "                label=\"üß† Zero-shot evaluation on Base Classes\"\n",
        "            )\n",
        "        finally:\n",
        "            del base_text_features\n",
        "            aggressive_cleanup()\n",
        "\n",
        "        # EVALUATION 2: Standard Novel Classes\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üîÑ EVALUATION 2: Standard Prompts on Novel Classes\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        novel_text_features = get_text_features_standard(model, novel_classes, device)\n",
        "        try:\n",
        "            results['standard_novel'] = eval(\n",
        "                model=model,\n",
        "                dataset=test_novel,\n",
        "                categories=novel_classes,\n",
        "                batch_size=64,\n",
        "                device=device,\n",
        "                text_features=novel_text_features,\n",
        "                label=\"üß† Zero-shot evaluation on Novel Classes\"\n",
        "            )\n",
        "        finally:\n",
        "            del novel_text_features\n",
        "            aggressive_cleanup()\n",
        "\n",
        "        # EVALUATIONS 3 & 4: LLM-Enhanced (if prompts available)\n",
        "        if has_llm_prompts:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"üîÑ EVALUATION 3: LLM-Enhanced Prompts on Base Classes\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            base_llm_features = get_llm_text_features(\n",
        "                model, generated_prompts_for_classes, base_classes, CLASS_NAMES, device\n",
        "            )\n",
        "            try:\n",
        "                results['llm_base'] = eval(\n",
        "                    model=model,\n",
        "                    dataset=test_base,\n",
        "                    categories=base_classes,\n",
        "                    batch_size=64,\n",
        "                    device=device,\n",
        "                    text_features=base_llm_features,\n",
        "                    label=\"üå∏ Zero-shot eval with LLM prompts on Base Classes\"\n",
        "                )\n",
        "            finally:\n",
        "                del base_llm_features\n",
        "                aggressive_cleanup()\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"üîÑ EVALUATION 4: LLM-Enhanced Prompts on Novel Classes\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            novel_llm_features = get_llm_text_features(\n",
        "                model, generated_prompts_for_classes, novel_classes, CLASS_NAMES, device\n",
        "            )\n",
        "            try:\n",
        "                results['llm_novel'] = eval(\n",
        "                    model=model,\n",
        "                    dataset=test_novel,\n",
        "                    categories=novel_classes,\n",
        "                    batch_size=64,\n",
        "                    device=device,\n",
        "                    text_features=novel_llm_features,\n",
        "                    label=\"üå∏ Zero-shot eval with LLM prompts on Novel Classes\"\n",
        "                )\n",
        "            finally:\n",
        "                del novel_llm_features\n",
        "                aggressive_cleanup()\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Main evaluation suite function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "7cIZiEPU9wE4",
        "outputId": "85ec02c0-0294-465f-d26a-999664fd0ad0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Main evaluation suite function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Results\n",
        "def analyze_results(results):\n",
        "    \"\"\"\n",
        "    Analyze and display comprehensive results from all evaluations.\n",
        "\n",
        "    Calculates harmonic means between base and novel class performance,\n",
        "    which is a standard metric for few-shot learning evaluation.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary containing evaluation results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä COMPREHENSIVE RESULTS ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    def harmonic_mean(base_acc, novel_acc):\n",
        "        \"\"\"Calculate harmonic mean of two accuracies\"\"\"\n",
        "        if base_acc > 0 and novel_acc > 0:\n",
        "            return 2 / (1/base_acc + 1/novel_acc)\n",
        "        return 0\n",
        "\n",
        "    # Standard prompts analysis\n",
        "    if 'standard_base' in results and 'standard_novel' in results:\n",
        "        base_top1 = results['standard_base']['top1']\n",
        "        novel_top1 = results['standard_novel']['top1']\n",
        "        std_hm = harmonic_mean(base_top1, novel_top1)\n",
        "\n",
        "        print(\"üî§ STANDARD PROMPTS RESULTS:\")\n",
        "        print(f\"   üìà Harmonic Mean (Top-1): {std_hm*100:.2f}%\")\n",
        "        print(f\"   üéØ Base Classes Top-1:    {base_top1*100:.2f}%\")\n",
        "        print(f\"   üÜï Novel Classes Top-1:   {novel_top1*100:.2f}%\")\n",
        "        print(f\"   üìä Base Classes Top-5:    {results['standard_base']['top5']*100:.2f}%\")\n",
        "        print(f\"   üìä Novel Classes Top-5:   {results['standard_novel']['top5']*100:.2f}%\")\n",
        "        print()\n",
        "\n",
        "    # LLM-enhanced prompts analysis\n",
        "    if 'llm_base' in results and 'llm_novel' in results:\n",
        "        base_top1_llm = results['llm_base']['top1']\n",
        "        novel_top1_llm = results['llm_novel']['top1']\n",
        "        llm_hm = harmonic_mean(base_top1_llm, novel_top1_llm)\n",
        "\n",
        "        print(\"ü§ñ LLM-ENHANCED PROMPTS RESULTS:\")\n",
        "        print(f\"   üìà Harmonic Mean (Top-1): {llm_hm*100:.2f}%\")\n",
        "        print(f\"   üéØ Base Classes Top-1:    {base_top1_llm*100:.2f}%\")\n",
        "        print(f\"   üÜï Novel Classes Top-1:   {novel_top1_llm*100:.2f}%\")\n",
        "        print(f\"   üìä Base Classes Top-5:    {results['llm_base']['top5']*100:.2f}%\")\n",
        "        print(f\"   üìä Novel Classes Top-5:   {results['llm_novel']['top5']*100:.2f}%\")\n",
        "        print()\n",
        "\n",
        "    # Improvement analysis\n",
        "    if all(key in results for key in ['standard_base', 'standard_novel', 'llm_base', 'llm_novel']):\n",
        "        base_improvement = (base_top1_llm - base_top1) * 100\n",
        "        novel_improvement = (novel_top1_llm - novel_top1) * 100\n",
        "        hm_improvement = (llm_hm - std_hm) * 100\n",
        "\n",
        "        print(\"üìà IMPROVEMENT WITH LLM PROMPTS:\")\n",
        "        print(f\"   üéØ Base Classes:    {base_improvement:+.2f} percentage points\")\n",
        "        print(f\"   üÜï Novel Classes:   {novel_improvement:+.2f} percentage points\")\n",
        "        print(f\"   üìà Harmonic Mean:   {hm_improvement:+.2f} percentage points\")\n",
        "        print()\n",
        "\n",
        "    # Confidence gap analysis\n",
        "    print(\"üîç CONFIDENCE GAP ANALYSIS:\")\n",
        "    for eval_name, eval_results in results.items():\n",
        "        eval_display = eval_name.replace('_', ' ').title()\n",
        "        print(f\"\\n   {eval_display}:\")\n",
        "        print(f\"     ‚úÖ Top-1 Hit Gap:     {eval_results['avg_gap_top1_hit']:.2f}%\")\n",
        "        print(f\"     ‚ùå Top-5 Hit Gap:    {eval_results['avg_error_top5_hit']:.2f}%\")\n",
        "        print(f\"     ‚ùå Top-10 Hit Gap:   {eval_results['avg_error_top10_hit']:.2f}%\")\n",
        "        print(f\"     ‚ùå Top-10 Miss Gap:  {eval_results['avg_error_top10_miss']:.2f}%\")\n",
        "\n",
        "def save_results(results, filename=\"clip_evaluation_results.json\"):\n",
        "    \"\"\"Save results to JSON file for later analysis\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aUau_-YT_XmU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zTnF4b47HMA",
        "outputId": "0b5ee181-83d3-410d-988d-dd8905e37da4",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENAI_KEY not found in Colab secrets. Please add it to proceed.\n",
            "Google GenAI SDK configured successfully!\n",
            "Pydantic schema defined.\n"
          ]
        }
      ],
      "source": [
        "#@title Gemini setup\n",
        "# Load your API key from Colab secrets.\n",
        "try:\n",
        "    GENAI_API_KEY = userdata.get('GENAI_KEY')\n",
        "    genai.configure(api_key=GENAI_API_KEY)\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"GENAI_KEY not found in Colab secrets. Please add it to proceed.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during API key setup: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(\"Google GenAI SDK configured successfully!\")\n",
        "\n",
        "# Define the Pydantic schema for the LLM output\n",
        "class PromptDescriptions(BaseModel):\n",
        "    # This will hold a list of lists: [[desc1_flower1, desc2_flower1, ...], [desc1_flower2, ...], ...]\n",
        "    flower_descriptions: List[List[str]] = Field(\n",
        "        description=\"A list where each element is a list of 5 short descriptions for a specific flower.\"\n",
        "    )\n",
        "\n",
        "print(\"Pydantic schema defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqPWs3Wk8tM_",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 3. LLM Prompt Generation (Batched Calls)\n",
        "\n",
        "genai_model = genai.GenerativeModel('gemini-2.5-flash')  # Using Flash for speed\n",
        "\n",
        "generated_prompts_for_classes = {}  # {class_name: [list of 5 prompts]}\n",
        "prompt_batch_size = 13\n",
        "\n",
        "# Define schema using Pydantic\n",
        "class Prompts(BaseModel):\n",
        "    prompt1: str\n",
        "    prompt2: str\n",
        "    prompt3: str\n",
        "    prompt4: str\n",
        "    prompt5: str\n",
        "\n",
        "class PromptDescriptions(BaseModel):\n",
        "    flower_descriptions: List[Prompts]\n",
        "\n",
        "print(\"\\n--- Generating 5 prompts for each flower class using LLM ---\")\n",
        "print(\"This might take a few minutes depending on the number of classes and API response times.\")\n",
        "\n",
        "for i in tqdm(range(0, len(CLASS_NAMES), prompt_batch_size), desc=\"Generating prompts in batches\"):\n",
        "    current_batch_classes = CLASS_NAMES[i : min(i + prompt_batch_size, len(CLASS_NAMES))]\n",
        "\n",
        "    # Dynamically build the prompt for the current batch\n",
        "    prompt_batch = f\"\"\"You are a professional botanical photographer and creative writer for a visual nature magazine.\n",
        "\n",
        "Given a list of flower species, generate exactly 5 short and visually evocative descriptions per flower. Each description should be phrased as if accompanying a stunning photograph of the flower, aiming to evoke its unique beauty and essence for an AI model like CLIP.\n",
        "\n",
        "Each prompt must be:\n",
        "- Descriptive, with strong visual language (color, shape, textures, dimension of petals, feeling)\n",
        "- Distinct across the 5 prompts (no overlap or simple rewordings)\n",
        "- Specific to the given flower (no generic phrases)\n",
        "- Emphasize features that distinguish this flower from visually similar species\n",
        "- Realistic and natural (imagine you're writing the description of your own photo of the flower)\n",
        "\n",
        "Use the word 'flower' in at least 2 out of 5 prompts. Prioritize conciseness (max 20 words) and features that distinguish the flower.\n",
        "\n",
        "Return the output as a JSON object with a single key 'flower_descriptions', whose value is a list of objects. Each object corresponds to a flower and includes exactly 5 fields: 'prompt1' through 'prompt5' (all strings).\n",
        "\n",
        "Here are the flowers for this batch: {current_batch_classes} \"\"\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        response_llm = genai_model.generate_content(\n",
        "            prompt_batch,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                response_mime_type=\"application/json\",\n",
        "                response_schema=PromptDescriptions\n",
        "            )\n",
        "        )\n",
        "\n",
        "        parsed_response = PromptDescriptions.model_validate_json(response_llm.text)\n",
        "\n",
        "        for j, prompt_obj in enumerate(parsed_response.flower_descriptions):\n",
        "            flower_name = current_batch_classes[j]\n",
        "            generated_prompts_for_classes[flower_name] = [\n",
        "                prompt_obj.prompt1,\n",
        "                prompt_obj.prompt2,\n",
        "                prompt_obj.prompt3,\n",
        "                prompt_obj.prompt4,\n",
        "                prompt_obj.prompt5,\n",
        "            ]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError generating prompts for batch starting with '{current_batch_classes[0]}': {e}\")\n",
        "        print(f\"Raw response (if available): {response_llm.text if 'response_llm' in locals() else 'N/A'}\")\n",
        "\n",
        "print(\"\\nLLM prompt generation complete.\")\n",
        "\n",
        "with open(\"generated_prompts.json\", \"w\") as f:\n",
        "    json.dump(generated_prompts_for_classes, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Evaluation\n",
        "results = run_evaluation_suite()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "imR1Bvc8GS1i",
        "outputId": "3a101d6b-7ac5-421b-de6b-737761936cef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting memory-optimized CLIP evaluation suite...\n",
            "‚úÖ Found generated prompts file\n",
            "üìä Loaded prompts for 102 classes\n",
            "ü§ñ Loading CLIP model: ViT-B/16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 335M/335M [00:05<00:00, 61.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CLIP model loaded on cuda\n",
            "üì± Using device: cuda\n",
            "\n",
            "üì• Loading and preparing datasets...\n",
            "üì• Downloading and loading Flowers102 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345M/345M [00:13<00:00, 26.2MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 502/502 [00:00<00:00, 786kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15.0k/15.0k [00:00<00:00, 24.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded - Train: 1020, Val: 1020, Test: 6149\n",
            "üìä Class split - Base: 51, Novel: 51\n",
            "üìä Data split - Base samples: 510, Novel samples: 510\n",
            "üìä Data split - Base samples: 510, Novel samples: 510\n",
            "üìä Data split - Base samples: 2473, Novel samples: 3676\n",
            "üìä Dataset prepared:\n",
            "   Base classes: 51 Novel classes: 51\n",
            "   Test base samples: 2473 Test novel samples: 3676\n",
            "\n",
            "============================================================\n",
            "üîÑ EVALUATION 1: Standard Prompts on Base Classes\n",
            "============================================================\n",
            "üìù Generated 51 standard prompts\n",
            "üìÑ Example prompt: 'a photo of a pink primrose, a type of flower.'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Base Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:27<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Total samples evaluated: 2473\n",
            "\n",
            "‚úÖ Top-1 Accuracy:      71.25%\n",
            "‚úÖ Top-5 Accuracy:      90.90%\n",
            "‚úÖ Top-10 Accuracy:     97.53%\n",
            "‚úÖ Avg. Conf. Gap (Top-1 hit):      3.27%\n",
            "‚ùå Avg. Conf. Gap (Top-5 hit):     1.67%\n",
            "‚ùå Avg. Conf. Gap (Top-10 hit):    3.88%\n",
            "‚ùå Avg. Conf. Gap (Beyond top-10): 5.01%\n",
            "\n",
            "============================================================\n",
            "üîÑ EVALUATION 2: Standard Prompts on Novel Classes\n",
            "============================================================\n",
            "üìù Generated 51 standard prompts\n",
            "üìÑ Example prompt: 'a photo of a wild pansy, a type of flower.'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:39<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Total samples evaluated: 3676\n",
            "\n",
            "‚úÖ Top-1 Accuracy:      78.26%\n",
            "‚úÖ Top-5 Accuracy:      89.15%\n",
            "‚úÖ Top-10 Accuracy:     92.79%\n",
            "‚úÖ Avg. Conf. Gap (Top-1 hit):      3.68%\n",
            "‚ùå Avg. Conf. Gap (Top-5 hit):     1.37%\n",
            "‚ùå Avg. Conf. Gap (Top-10 hit):    3.45%\n",
            "‚ùå Avg. Conf. Gap (Beyond top-10): 5.70%\n",
            "\n",
            "============================================================\n",
            "üîÑ EVALUATION 3: LLM-Enhanced Prompts on Base Classes\n",
            "============================================================\n",
            "ü§ñ Processing LLM-generated prompts for 51 classes...\n",
            "üìù Processing 5 prompts for 'pink primrose'\n",
            "üìù Processing 5 prompts for 'hard-leaved pocket orchid'\n",
            "üìù Processing 5 prompts for 'canterbury bells'\n",
            "üìù Processing 5 prompts for 'sweet pea'\n",
            "üìù Processing 5 prompts for 'english marigold'\n",
            "üìù Processing 5 prompts for 'tiger lily'\n",
            "üìù Processing 5 prompts for 'moon orchid'\n",
            "üìù Processing 5 prompts for 'bird of paradise'\n",
            "üìù Processing 5 prompts for 'monkshood'\n",
            "üìù Processing 5 prompts for 'globe thistle'\n",
            "üìù Processing 5 prompts for 'snapdragon'\n",
            "üìù Processing 5 prompts for 'colt's foot'\n",
            "üìù Processing 5 prompts for 'king protea'\n",
            "üìù Processing 5 prompts for 'spear thistle'\n",
            "üìù Processing 5 prompts for 'yellow iris'\n",
            "üìù Processing 5 prompts for 'globe-flower'\n",
            "üìù Processing 5 prompts for 'purple coneflower'\n",
            "üìù Processing 5 prompts for 'peruvian lily'\n",
            "üìù Processing 5 prompts for 'balloon flower'\n",
            "üìù Processing 5 prompts for 'giant white arum lily'\n",
            "üìù Processing 5 prompts for 'fire lily'\n",
            "üìù Processing 5 prompts for 'pincushion flower'\n",
            "üìù Processing 5 prompts for 'fritillary'\n",
            "üìù Processing 5 prompts for 'red ginger'\n",
            "üìù Processing 5 prompts for 'grape hyacinth'\n",
            "üìù Processing 5 prompts for 'corn poppy'\n",
            "üìù Processing 5 prompts for 'prince of wales feathers'\n",
            "üìù Processing 5 prompts for 'stemless gentian'\n",
            "üìù Processing 5 prompts for 'artichoke'\n",
            "üìù Processing 5 prompts for 'sweet william'\n",
            "üìù Processing 5 prompts for 'carnation'\n",
            "üìù Processing 5 prompts for 'garden phlox'\n",
            "üìù Processing 5 prompts for 'love in the mist'\n",
            "üìù Processing 5 prompts for 'mexican aster'\n",
            "üìù Processing 5 prompts for 'alpine sea holly'\n",
            "üìù Processing 5 prompts for 'ruby-lipped cattleya'\n",
            "üìù Processing 5 prompts for 'cape flower'\n",
            "üìù Processing 5 prompts for 'great masterwort'\n",
            "üìù Processing 5 prompts for 'siam tulip'\n",
            "üìù Processing 5 prompts for 'lenten rose'\n",
            "üìù Processing 5 prompts for 'barbeton daisy'\n",
            "üìù Processing 5 prompts for 'daffodil'\n",
            "üìù Processing 5 prompts for 'sword lily'\n",
            "üìù Processing 5 prompts for 'poinsettia'\n",
            "üìù Processing 5 prompts for 'bolero deep blue'\n",
            "üìù Processing 5 prompts for 'wallflower'\n",
            "üìù Processing 5 prompts for 'marigold'\n",
            "üìù Processing 5 prompts for 'buttercup'\n",
            "üìù Processing 5 prompts for 'oxeye daisy'\n",
            "üìù Processing 5 prompts for 'common dandelion'\n",
            "üìù Processing 5 prompts for 'petunia'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üå∏ Zero-shot eval with LLM prompts on Base Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:26<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Total samples evaluated: 2473\n",
            "\n",
            "‚úÖ Top-1 Accuracy:      79.18%\n",
            "‚úÖ Top-5 Accuracy:      94.99%\n",
            "‚úÖ Top-10 Accuracy:     96.24%\n",
            "‚úÖ Avg. Conf. Gap (Top-1 hit):      3.85%\n",
            "‚ùå Avg. Conf. Gap (Top-5 hit):     1.51%\n",
            "‚ùå Avg. Conf. Gap (Top-10 hit):    4.15%\n",
            "‚ùå Avg. Conf. Gap (Beyond top-10): 7.14%\n",
            "\n",
            "============================================================\n",
            "üîÑ EVALUATION 4: LLM-Enhanced Prompts on Novel Classes\n",
            "============================================================\n",
            "ü§ñ Processing LLM-generated prompts for 51 classes...\n",
            "üìù Processing 5 prompts for 'wild pansy'\n",
            "üìù Processing 5 prompts for 'primula'\n",
            "üìù Processing 5 prompts for 'sunflower'\n",
            "üìù Processing 5 prompts for 'pelargonium'\n",
            "üìù Processing 5 prompts for 'bishop of llandaff'\n",
            "üìù Processing 5 prompts for 'gaura'\n",
            "üìù Processing 5 prompts for 'geranium'\n",
            "üìù Processing 5 prompts for 'orange dahlia'\n",
            "üìù Processing 5 prompts for 'pink-yellow dahlia?'\n",
            "üìù Processing 5 prompts for 'cautleya spicata'\n",
            "üìù Processing 5 prompts for 'japanese anemone'\n",
            "üìù Processing 5 prompts for 'black-eyed susan'\n",
            "üìù Processing 5 prompts for 'silverbush'\n",
            "üìù Processing 5 prompts for 'californian poppy'\n",
            "üìù Processing 5 prompts for 'osteospermum'\n",
            "üìù Processing 5 prompts for 'spring crocus'\n",
            "üìù Processing 5 prompts for 'bearded iris'\n",
            "üìù Processing 5 prompts for 'windflower'\n",
            "üìù Processing 5 prompts for 'tree poppy'\n",
            "üìù Processing 5 prompts for 'gazania'\n",
            "üìù Processing 5 prompts for 'azalea'\n",
            "üìù Processing 5 prompts for 'water lily'\n",
            "üìù Processing 5 prompts for 'rose'\n",
            "üìù Processing 5 prompts for 'thorn apple'\n",
            "üìù Processing 5 prompts for 'morning glory'\n",
            "üìù Processing 5 prompts for 'passion flower'\n",
            "üìù Processing 5 prompts for 'lotus'\n",
            "üìù Processing 5 prompts for 'toad lily'\n",
            "üìù Processing 5 prompts for 'anthurium'\n",
            "üìù Processing 5 prompts for 'frangipani'\n",
            "üìù Processing 5 prompts for 'clematis'\n",
            "üìù Processing 5 prompts for 'hibiscus'\n",
            "üìù Processing 5 prompts for 'columbine'\n",
            "üìù Processing 5 prompts for 'desert-rose'\n",
            "üìù Processing 5 prompts for 'tree mallow'\n",
            "üìù Processing 5 prompts for 'magnolia'\n",
            "üìù Processing 5 prompts for 'cyclamen'\n",
            "üìù Processing 5 prompts for 'watercress'\n",
            "üìù Processing 5 prompts for 'canna lily'\n",
            "üìù Processing 5 prompts for 'hippeastrum'\n",
            "üìù Processing 5 prompts for 'bee balm'\n",
            "üìù Processing 5 prompts for 'ball moss'\n",
            "üìù Processing 5 prompts for 'foxglove'\n",
            "üìù Processing 5 prompts for 'bougainvillea'\n",
            "üìù Processing 5 prompts for 'camellia'\n",
            "üìù Processing 5 prompts for 'mallow'\n",
            "üìù Processing 5 prompts for 'mexican petunia'\n",
            "üìù Processing 5 prompts for 'bromelia'\n",
            "üìù Processing 5 prompts for 'blanket flower'\n",
            "üìù Processing 5 prompts for 'trumpet creeper'\n",
            "üìù Processing 5 prompts for 'blackberry lily'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üå∏ Zero-shot eval with LLM prompts on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:40<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Total samples evaluated: 3676\n",
            "\n",
            "‚úÖ Top-1 Accuracy:      80.66%\n",
            "‚úÖ Top-5 Accuracy:      92.19%\n",
            "‚úÖ Top-10 Accuracy:     93.25%\n",
            "‚úÖ Avg. Conf. Gap (Top-1 hit):      4.32%\n",
            "‚ùå Avg. Conf. Gap (Top-5 hit):     1.93%\n",
            "‚ùå Avg. Conf. Gap (Top-10 hit):    4.39%\n",
            "‚ùå Avg. Conf. Gap (Beyond top-10): 13.61%\n",
            "üßπ Cleaning up CLIP model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show results\n",
        "analyze_results(results=results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "94W5dlyoGcc1",
        "outputId": "046d1922-bfbd-4cea-b872-34658619500d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìä COMPREHENSIVE RESULTS ANALYSIS\n",
            "============================================================\n",
            "üî§ STANDARD PROMPTS RESULTS:\n",
            "   üìà Harmonic Mean (Top-1): 74.59%\n",
            "   üéØ Base Classes Top-1:    71.25%\n",
            "   üÜï Novel Classes Top-1:   78.26%\n",
            "   üìä Base Classes Top-5:    90.90%\n",
            "   üìä Novel Classes Top-5:   89.15%\n",
            "\n",
            "ü§ñ LLM-ENHANCED PROMPTS RESULTS:\n",
            "   üìà Harmonic Mean (Top-1): 79.91%\n",
            "   üéØ Base Classes Top-1:    79.18%\n",
            "   üÜï Novel Classes Top-1:   80.66%\n",
            "   üìä Base Classes Top-5:    94.99%\n",
            "   üìä Novel Classes Top-5:   92.19%\n",
            "\n",
            "üìà IMPROVEMENT WITH LLM PROMPTS:\n",
            "   üéØ Base Classes:    +7.93 percentage points\n",
            "   üÜï Novel Classes:   +2.39 percentage points\n",
            "   üìà Harmonic Mean:   +5.32 percentage points\n",
            "\n",
            "üîç CONFIDENCE GAP ANALYSIS:\n",
            "\n",
            "   Standard Base:\n",
            "     ‚úÖ Top-1 Hit Gap:     3.27%\n",
            "     ‚ùå Top-5 Hit Gap:    1.67%\n",
            "     ‚ùå Top-10 Hit Gap:   3.88%\n",
            "     ‚ùå Top-10 Miss Gap:  5.01%\n",
            "\n",
            "   Standard Novel:\n",
            "     ‚úÖ Top-1 Hit Gap:     3.68%\n",
            "     ‚ùå Top-5 Hit Gap:    1.37%\n",
            "     ‚ùå Top-10 Hit Gap:   3.45%\n",
            "     ‚ùå Top-10 Miss Gap:  5.70%\n",
            "\n",
            "   Llm Base:\n",
            "     ‚úÖ Top-1 Hit Gap:     3.85%\n",
            "     ‚ùå Top-5 Hit Gap:    1.51%\n",
            "     ‚ùå Top-10 Hit Gap:   4.15%\n",
            "     ‚ùå Top-10 Miss Gap:  7.14%\n",
            "\n",
            "   Llm Novel:\n",
            "     ‚úÖ Top-1 Hit Gap:     4.32%\n",
            "     ‚ùå Top-5 Hit Gap:    1.93%\n",
            "     ‚ùå Top-10 Hit Gap:   4.39%\n",
            "     ‚ùå Top-10 Miss Gap:  13.61%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}