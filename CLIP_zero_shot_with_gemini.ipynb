{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYQiprE0pcVD"
      },
      "source": [
        "# CLIP zero-shot Evaluation\n",
        "This short notebook implements the dataset split into base and novel categories (see project assignment) and runs the zero-shot evaluation with CLIP.\n",
        "Feel free to copy the code contained in this notebook or to directly use this notebook as starting point for you project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UzXtFjhh7iOS",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b2cd75-af93-4b09-851f-8ae2dff0e477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai_clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#@title Dependencies Installation\n",
        "%pip install -q google-genai pydantic openai_clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QtqdSOr8qqOn",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b13e207-b166-43d2-af83-2855128f9cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All dependencies imported successfully!\n",
            "🔥 PyTorch version: 2.6.0+cu124\n",
            "🖼️ Torchvision version: 0.21.0+cu124\n",
            "🤖 CUDA available: True\n",
            "🎮 GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torchvision\n",
        "import clip\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import gc\n",
        "from torchvision import transforms\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "from torchvision.datasets import Flowers102 as dataset_used\n",
        "import contextlib\n",
        "import numpy as np\n",
        "\n",
        "print(\"✅ All dependencies imported successfully!\")\n",
        "print(f\"🔥 PyTorch version: {torch.__version__}\")\n",
        "print(f\"🖼️ Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"🤖 CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"🎮 GPU: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Classes setup\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
        "\n",
        "print(f\"📊 Total flower classes: {len(CLASS_NAMES)}\")\n",
        "print(f\"🌸 Sample classes: {CLASS_NAMES[:5]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "feUqkSTsvj3b",
        "outputId": "cd1a9654-6e1d-4e3b-d833-33ff3e997771"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Total flower classes: 102\n",
            "🌸 Sample classes: ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Memory management utils\n",
        "def aggressive_cleanup():\n",
        "    \"\"\"\n",
        "    Comprehensive memory cleanup function to prevent VRAM leaks.\n",
        "\n",
        "    This function:\n",
        "    - Forces Python garbage collection\n",
        "    - Empties CUDA cache\n",
        "    - Collects IPC (Inter-Process Communication) resources\n",
        "    - Synchronizes CUDA operations\n",
        "    \"\"\"\n",
        "    gc.collect()                    # Python garbage collection\n",
        "    torch.cuda.empty_cache()        # Clear CUDA cache\n",
        "    torch.cuda.ipc_collect()        # Clean up IPC resources\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()    # Wait for all CUDA operations to complete\n",
        "\n",
        "def print_memory_stats():\n",
        "    \"\"\"Display current GPU memory usage statistics\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3    # Convert to GB\n",
        "        print(f\"🔋 GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
        "    else:\n",
        "        print(\"❌ CUDA not available - running on CPU\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HvcFJ1LL7xO3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M_1CrUhZpVCq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Utils functions\n",
        "\n",
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\n",
        "    Args:\n",
        "        data_dir (str): Directory where the dataset will be stored.\n",
        "        transform (torch.Compose)\n",
        "    Returns:\n",
        "        tuple: A tuple containing the train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    print(\"📥 Downloading and loading Flowers102 dataset...\")\n",
        "    train = dataset_used(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = dataset_used(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = dataset_used(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    print(f\"✅ Dataset loaded - Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
        "    return train, val, test\n",
        "\n",
        "def base_novel_categories(dataset):\n",
        "    \"\"\"\n",
        "    Split all classes into base and novel categories.\n",
        "\n",
        "    Base classes: First half of classes (for training/fine-tuning scenarios)\n",
        "    Novel classes: Second half of classes (for zero-shot evaluation)\n",
        "\n",
        "    Args:\n",
        "        dataset: PyTorch dataset object\n",
        "\n",
        "    Returns:\n",
        "        base_classes, novel_classes: Lists of class indices\n",
        "    \"\"\"\n",
        "    all_classes = set(dataset._labels)\n",
        "    num_classes = len(all_classes)\n",
        "\n",
        "    # Split classes 50/50\n",
        "    base_classes = list(range(num_classes))[:num_classes//2]\n",
        "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
        "\n",
        "    print(f\"📊 Class split - Base: {len(base_classes)}, Novel: {len(novel_classes)}\")\n",
        "    return base_classes, novel_classes\n",
        "\n",
        "def split_data(dataset, base_classes):\n",
        "    \"\"\"\n",
        "    Split dataset samples based on base/novel class membership.\n",
        "\n",
        "    Args:\n",
        "        dataset: PyTorch dataset\n",
        "        base_classes: List of base class indices\n",
        "\n",
        "    Returns:\n",
        "        base_dataset, novel_dataset: Subsets containing only base/novel samples\n",
        "    \"\"\"\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    # Iterate through all samples and categorize by class\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    # Create dataset subsets\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "\n",
        "    print(f\"📊 Data split - Base samples: {len(base_dataset)}, Novel samples: {len(novel_dataset)}\")\n",
        "    return base_dataset, novel_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CLIP context manager\n",
        "@contextlib.contextmanager\n",
        "def clip_model_context(model_name=\"ViT-B/16\"):\n",
        "    \"\"\"\n",
        "    Context manager for CLIP model to ensure proper cleanup.\n",
        "\n",
        "    This ensures that:\n",
        "    - Model is properly loaded on the correct device\n",
        "    - Model is set to evaluation mode\n",
        "    - Memory is cleaned up after use, even if errors occur\n",
        "\n",
        "    Args:\n",
        "        model_name: CLIP model variant to load\n",
        "\n",
        "    Yields:\n",
        "        model: CLIP model\n",
        "        preprocess: Image preprocessing function\n",
        "        device: Device (cuda/cpu) the model is loaded on\n",
        "    \"\"\"\n",
        "    print(f\"🤖 Loading CLIP model: {model_name}\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Load CLIP model and preprocessing\n",
        "    model, preprocess = clip.load(model_name, device=device)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    print(f\"✅ CLIP model loaded on {device}\")\n",
        "\n",
        "    try:\n",
        "        yield model, preprocess, device\n",
        "    finally:\n",
        "        # Ensure cleanup happens even if errors occur\n",
        "        print(\"🧹 Cleaning up CLIP model...\")\n",
        "        del model\n",
        "        aggressive_cleanup()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QDBGk6Z68gbQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluation function\n",
        "@torch.no_grad()\n",
        "def eval(model, dataset, categories, batch_size, device, text_features, label=\"\"):\n",
        "    \"\"\"\n",
        "    Memory-optimized evaluation function for CLIP zero-shot classification.\n",
        "\n",
        "    Computes:\n",
        "    - Top-1, Top-5, Top-10 accuracies\n",
        "    - Confidence gaps between predicted and true classes\n",
        "    - Splits confidence gaps by error type (top-1 hit, top-5 hit, etc.)\n",
        "\n",
        "    Args:\n",
        "        model: CLIP model\n",
        "        dataset: PyTorch dataset to evaluate\n",
        "        categories: List of class indices for this evaluation\n",
        "        batch_size: Batch size for evaluation\n",
        "        device: Device to run on\n",
        "        text_features: Pre-computed text embeddings\n",
        "        label: Description label for progress bar\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with accuracy metrics and confidence gaps\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Create mapping from original class IDs to contiguous indices (0, 1, 2, ...)\n",
        "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "    # Create dataloader (num_workers=0 to avoid subprocess memory issues in Colab)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
        "    )\n",
        "\n",
        "    # Initialize counters\n",
        "    correct_top1 = 0\n",
        "    correct_top5 = 0\n",
        "    correct_top10 = 0\n",
        "    total = 0\n",
        "\n",
        "    # Lists to store confidence gaps for different error categories\n",
        "    gap_top1_hit = []      # When prediction is correct\n",
        "    gap_top5_hit = []      # When true class is in top-5 but not top-1\n",
        "    gap_top10_hit = []     # When true class is in top-10 but not top-5\n",
        "    gap_top10_miss = []    # When true class is not in top-10\n",
        "\n",
        "    try:\n",
        "        for batch_idx, (images, targets) in enumerate(tqdm(dataloader, desc=label)):\n",
        "            # Remap targets to contiguous space and move to device\n",
        "            targets = torch.tensor([contig_cat2idx[t.item()] for t in targets], dtype=torch.long).to(device)\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Encode images\n",
        "            image_features = model.encode_image(images)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Compute similarities between images and text\n",
        "            similarities = image_features @ text_features.T\n",
        "\n",
        "            # Get top-10 predictions\n",
        "            top10 = similarities.topk(10, dim=-1)\n",
        "            top10_indices = top10.indices  # Class predictions\n",
        "            top10_values = top10.values    # Confidence scores\n",
        "\n",
        "            # Calculate accuracies\n",
        "            correct_top1 += (top10_indices[:, 0] == targets).sum().item()\n",
        "            correct_top5 += sum([targets[i] in top10_indices[i, :5] for i in range(len(targets))])\n",
        "            correct_top10 += sum([targets[i] in top10_indices[i, :10] for i in range(len(targets))])\n",
        "\n",
        "            # Calculate confidence gaps for each sample\n",
        "            for i in range(len(targets)):\n",
        "                true_idx = targets[i].item()\n",
        "                pred_conf = top10_values[i, 0].item()  # Highest prediction confidence\n",
        "                true_conf = similarities[i, true_idx].item()  # True class confidence\n",
        "\n",
        "                # Categorize by error type\n",
        "                if top10_indices[i, 0].item() == true_idx:\n",
        "                    # Correct prediction - gap between 1st and 2nd choice\n",
        "                    second_conf = top10_values[i, 1].item()\n",
        "                    gap_top1_hit.append((pred_conf - second_conf) * 100)\n",
        "                elif true_idx in top10_indices[i, 1:5]:\n",
        "                    # True class in top-5 but not top-1\n",
        "                    gap_top5_hit.append((pred_conf - true_conf) * 100)\n",
        "                elif true_idx in top10_indices[i, 5:10]:\n",
        "                    # True class in top-10 but not top-5\n",
        "                    gap_top10_hit.append((pred_conf - true_conf) * 100)\n",
        "                else:\n",
        "                    # True class not in top-10\n",
        "                    gap_top10_miss.append((pred_conf - true_conf) * 100)\n",
        "\n",
        "            total += targets.size(0)\n",
        "\n",
        "            # Clean up batch tensors immediately to save memory\n",
        "            del images, targets, image_features, similarities, top10, top10_indices, top10_values\n",
        "\n",
        "            # Periodic cleanup during long evaluations\n",
        "            if batch_idx % 10 == 0:\n",
        "                aggressive_cleanup()\n",
        "\n",
        "    finally:\n",
        "        # Clean up dataloader\n",
        "        del dataloader\n",
        "        aggressive_cleanup()\n",
        "\n",
        "    # Calculate final metrics\n",
        "    top1_acc = correct_top1 / total\n",
        "    top5_acc = correct_top5 / total\n",
        "    top10_acc = correct_top10 / total\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n📊 Total samples evaluated: {total}\\n\")\n",
        "    print(f\"✅ Top-1 Accuracy:      {top1_acc*100:.2f}%\")\n",
        "    print(f\"✅ Top-5 Accuracy:      {top5_acc*100:.2f}%\")\n",
        "    print(f\"✅ Top-10 Accuracy:     {top10_acc*100:.2f}%\")\n",
        "    print(f\"✅ Avg. Conf. Gap (Top-1 hit):      {safe_mean(gap_top1_hit):.2f}%\")\n",
        "    print(f\"❌ Avg. Conf. Gap (Top-5 hit):     {safe_mean(gap_top5_hit):.2f}%\")\n",
        "    print(f\"❌ Avg. Conf. Gap (Top-10 hit):    {safe_mean(gap_top10_hit):.2f}%\")\n",
        "    print(f\"❌ Avg. Conf. Gap (Beyond top-10): {safe_mean(gap_top10_miss):.2f}%\")\n",
        "\n",
        "    return {\n",
        "        \"top1\": top1_acc,\n",
        "        \"top5\": top5_acc,\n",
        "        \"top10\": top10_acc,\n",
        "        \"avg_gap_top1_hit\": safe_mean(gap_top1_hit),\n",
        "        \"avg_error_top5_hit\": safe_mean(gap_top5_hit),\n",
        "        \"avg_error_top10_hit\": safe_mean(gap_top10_hit),\n",
        "        \"avg_error_top10_miss\": safe_mean(gap_top10_miss),\n",
        "    }"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mrRHjqc68W2v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Text features functions\n",
        "def get_text_features_standard(model, class_ids, device):\n",
        "    \"\"\"\n",
        "    Generate standard CLIP text features using simple template prompts.\n",
        "\n",
        "    Uses the template: \"a photo of a {class_name}, a type of flower.\"\n",
        "\n",
        "    Args:\n",
        "        model: CLIP model\n",
        "        class_ids: List of class indices to generate features for\n",
        "        device: Device to run computation on\n",
        "\n",
        "    Returns:\n",
        "        text_features: Normalized text embeddings tensor\n",
        "    \"\"\"\n",
        "    # Create simple template prompts\n",
        "    prompts = [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in class_ids]\n",
        "    print(f\"📝 Generated {len(prompts)} standard prompts\")\n",
        "    print(f\"📄 Example prompt: '{prompts[0]}'\")\n",
        "\n",
        "    # Tokenize prompts\n",
        "    text_inputs = clip.tokenize(prompts).to(device)\n",
        "\n",
        "    try:\n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "            text_features = model.encode_text(text_inputs)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Create a copy to ensure we don't keep references\n",
        "        result = text_features.clone()\n",
        "        return result\n",
        "\n",
        "    finally:\n",
        "        # Clean up intermediate tensors\n",
        "        del text_inputs, text_features\n",
        "        aggressive_cleanup()\n",
        "\n",
        "def get_llm_text_features(model, prompt_dict, class_ids, class_names, device):\n",
        "    \"\"\"\n",
        "    Generate text features from LLM-generated prompts.\n",
        "\n",
        "    For each class, uses multiple detailed prompts generated by an LLM,\n",
        "    then averages their embeddings to get a richer representation.\n",
        "\n",
        "    Args:\n",
        "        model: CLIP model\n",
        "        prompt_dict: Dictionary mapping class names to lists of prompts\n",
        "        class_ids: List of class indices\n",
        "        class_names: List of all class names\n",
        "        device: Device to run computation on\n",
        "\n",
        "    Returns:\n",
        "        text_features: Tensor of averaged normalized embeddings per class\n",
        "    \"\"\"\n",
        "    text_features = []\n",
        "\n",
        "    print(f\"🤖 Processing LLM-generated prompts for {len(class_ids)} classes...\")\n",
        "\n",
        "    for c in class_ids:\n",
        "        class_name = class_names[c]\n",
        "        prompts = prompt_dict[class_name]\n",
        "\n",
        "        print(f\"📝 Processing {len(prompts)} prompts for '{class_name}'\")\n",
        "\n",
        "        # Tokenize all prompts for this class\n",
        "        text_inputs = clip.tokenize(prompts).to(device)\n",
        "\n",
        "        try:\n",
        "            # Generate embeddings for all prompts\n",
        "            with torch.no_grad():\n",
        "                embeddings = model.encode_text(text_inputs)\n",
        "                embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                # Average across all prompts for this class\n",
        "                mean_embedding = embeddings.mean(dim=0)\n",
        "                mean_embedding = mean_embedding / mean_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                # Store the averaged embedding\n",
        "                text_features.append(mean_embedding.clone())\n",
        "\n",
        "        finally:\n",
        "            # Clean up intermediate tensors\n",
        "            del text_inputs, embeddings, mean_embedding\n",
        "            aggressive_cleanup()\n",
        "\n",
        "    return torch.stack(text_features).to(device)\n",
        "\n",
        "def safe_mean(arr):\n",
        "    \"\"\"Safely compute mean, returning 0 if array is empty\"\"\"\n",
        "    return np.mean(arr) if arr else 0.0"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vCVSIRgR8zzT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluation suite\n",
        "def run_evaluation_suite():\n",
        "    \"\"\"\n",
        "    Run complete evaluation suite with proper memory management.\n",
        "\n",
        "    Evaluations performed:\n",
        "    1. Standard prompts on base classes\n",
        "    2. Standard prompts on novel classes\n",
        "    3. LLM-enhanced prompts on base classes (if available)\n",
        "    4. LLM-enhanced prompts on novel classes (if available)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing all evaluation results\n",
        "    \"\"\"\n",
        "    print(\"🚀 Starting memory-optimized CLIP evaluation suite...\")\n",
        "\n",
        "    # Try to load LLM-generated prompts\n",
        "    try:\n",
        "        with open(\"generated_prompts.json\", \"r\") as f:\n",
        "            generated_prompts_for_classes = json.load(f)\n",
        "        has_llm_prompts = True\n",
        "        print(\"✅ Found generated prompts file\")\n",
        "        print(f\"📊 Loaded prompts for {len(generated_prompts_for_classes)} classes\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"❌ No generated prompts found. Will only run standard evaluation.\")\n",
        "        has_llm_prompts = False\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Use context manager for proper model lifecycle management\n",
        "    with clip_model_context(\"ViT-B/16\") as (model, preprocess, device):\n",
        "        print(f\"📱 Using device: {device}\")\n",
        "\n",
        "        # Load and prepare datasets\n",
        "        print(\"\\n📥 Loading and preparing datasets...\")\n",
        "        train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "        base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "        # Split datasets by base/novel classes\n",
        "        train_base, _ = split_data(train_set, base_classes)\n",
        "        val_base, _ = split_data(val_set, base_classes)\n",
        "        test_base, test_novel = split_data(test_set, base_classes)\n",
        "\n",
        "        print(f\"📊 Dataset prepared:\")\n",
        "        print(f\"   Base classes: {len(base_classes)} Novel classes: {len(novel_classes)}\")\n",
        "        print(f\"   Test base samples: {len(test_base)} Test novel samples: {len(test_novel)}\")\n",
        "\n",
        "        # EVALUATION 1: Standard Base Classes\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"🔄 EVALUATION 1: Standard Prompts on Base Classes\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        base_text_features = get_text_features_standard(model, base_classes, device)\n",
        "        try:\n",
        "            results['standard_base'] = eval(\n",
        "                model=model,\n",
        "                dataset=test_base,\n",
        "                categories=base_classes,\n",
        "                batch_size=64,  # Conservative batch size for Colab\n",
        "                device=device,\n",
        "                text_features=base_text_features,\n",
        "                label=\"🧠 Zero-shot evaluation on Base Classes\"\n",
        "            )\n",
        "        finally:\n",
        "            del base_text_features\n",
        "            aggressive_cleanup()\n",
        "\n",
        "        # EVALUATION 2: Standard Novel Classes\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"🔄 EVALUATION 2: Standard Prompts on Novel Classes\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        novel_text_features = get_text_features_standard(model, novel_classes, device)\n",
        "        try:\n",
        "            results['standard_novel'] = eval(\n",
        "                model=model,\n",
        "                dataset=test_novel,\n",
        "                categories=novel_classes,\n",
        "                batch_size=64,\n",
        "                device=device,\n",
        "                text_features=novel_text_features,\n",
        "                label=\"🧠 Zero-shot evaluation on Novel Classes\"\n",
        "            )\n",
        "        finally:\n",
        "            del novel_text_features\n",
        "            aggressive_cleanup()\n",
        "\n",
        "        # EVALUATIONS 3 & 4: LLM-Enhanced (if prompts available)\n",
        "        if has_llm_prompts:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"🔄 EVALUATION 3: LLM-Enhanced Prompts on Base Classes\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            base_llm_features = get_llm_text_features(\n",
        "                model, generated_prompts_for_classes, base_classes, CLASS_NAMES, device\n",
        "            )\n",
        "            try:\n",
        "                results['llm_base'] = eval(\n",
        "                    model=model,\n",
        "                    dataset=test_base,\n",
        "                    categories=base_classes,\n",
        "                    batch_size=64,\n",
        "                    device=device,\n",
        "                    text_features=base_llm_features,\n",
        "                    label=\"🌸 Zero-shot eval with LLM prompts on Base Classes\"\n",
        "                )\n",
        "            finally:\n",
        "                del base_llm_features\n",
        "                aggressive_cleanup()\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"🔄 EVALUATION 4: LLM-Enhanced Prompts on Novel Classes\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            novel_llm_features = get_llm_text_features(\n",
        "                model, generated_prompts_for_classes, novel_classes, CLASS_NAMES, device\n",
        "            )\n",
        "            try:\n",
        "                results['llm_novel'] = eval(\n",
        "                    model=model,\n",
        "                    dataset=test_novel,\n",
        "                    categories=novel_classes,\n",
        "                    batch_size=64,\n",
        "                    device=device,\n",
        "                    text_features=novel_llm_features,\n",
        "                    label=\"🌸 Zero-shot eval with LLM prompts on Novel Classes\"\n",
        "                )\n",
        "            finally:\n",
        "                del novel_llm_features\n",
        "                aggressive_cleanup()\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"✅ Main evaluation suite function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "7cIZiEPU9wE4",
        "outputId": "85ec02c0-0294-465f-d26a-999664fd0ad0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Main evaluation suite function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Results\n",
        "def analyze_results(results):\n",
        "    \"\"\"\n",
        "    Analyze and display comprehensive results from all evaluations.\n",
        "\n",
        "    Calculates harmonic means between base and novel class performance,\n",
        "    which is a standard metric for few-shot learning evaluation.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary containing evaluation results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📊 COMPREHENSIVE RESULTS ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    def harmonic_mean(base_acc, novel_acc):\n",
        "        \"\"\"Calculate harmonic mean of two accuracies\"\"\"\n",
        "        if base_acc > 0 and novel_acc > 0:\n",
        "            return 2 / (1/base_acc + 1/novel_acc)\n",
        "        return 0\n",
        "\n",
        "    # Standard prompts analysis\n",
        "    if 'standard_base' in results and 'standard_novel' in results:\n",
        "        base_top1 = results['standard_base']['top1']\n",
        "        novel_top1 = results['standard_novel']['top1']\n",
        "        std_hm = harmonic_mean(base_top1, novel_top1)\n",
        "\n",
        "        print(\"🔤 STANDARD PROMPTS RESULTS:\")\n",
        "        print(f\"   📈 Harmonic Mean (Top-1): {std_hm*100:.2f}%\")\n",
        "        print(f\"   🎯 Base Classes Top-1:    {base_top1*100:.2f}%\")\n",
        "        print(f\"   🆕 Novel Classes Top-1:   {novel_top1*100:.2f}%\")\n",
        "        print(f\"   📊 Base Classes Top-5:    {results['standard_base']['top5']*100:.2f}%\")\n",
        "        print(f\"   📊 Novel Classes Top-5:   {results['standard_novel']['top5']*100:.2f}%\")\n",
        "        print()\n",
        "\n",
        "    # LLM-enhanced prompts analysis\n",
        "    if 'llm_base' in results and 'llm_novel' in results:\n",
        "        base_top1_llm = results['llm_base']['top1']\n",
        "        novel_top1_llm = results['llm_novel']['top1']\n",
        "        llm_hm = harmonic_mean(base_top1_llm, novel_top1_llm)\n",
        "\n",
        "        print(\"🤖 LLM-ENHANCED PROMPTS RESULTS:\")\n",
        "        print(f\"   📈 Harmonic Mean (Top-1): {llm_hm*100:.2f}%\")\n",
        "        print(f\"   🎯 Base Classes Top-1:    {base_top1_llm*100:.2f}%\")\n",
        "        print(f\"   🆕 Novel Classes Top-1:   {novel_top1_llm*100:.2f}%\")\n",
        "        print(f\"   📊 Base Classes Top-5:    {results['llm_base']['top5']*100:.2f}%\")\n",
        "        print(f\"   📊 Novel Classes Top-5:   {results['llm_novel']['top5']*100:.2f}%\")\n",
        "        print()\n",
        "\n",
        "    # Improvement analysis\n",
        "    if all(key in results for key in ['standard_base', 'standard_novel', 'llm_base', 'llm_novel']):\n",
        "        base_improvement = (base_top1_llm - base_top1) * 100\n",
        "        novel_improvement = (novel_top1_llm - novel_top1) * 100\n",
        "        hm_improvement = (llm_hm - std_hm) * 100\n",
        "\n",
        "        print(\"📈 IMPROVEMENT WITH LLM PROMPTS:\")\n",
        "        print(f\"   🎯 Base Classes:    {base_improvement:+.2f} percentage points\")\n",
        "        print(f\"   🆕 Novel Classes:   {novel_improvement:+.2f} percentage points\")\n",
        "        print(f\"   📈 Harmonic Mean:   {hm_improvement:+.2f} percentage points\")\n",
        "        print()\n",
        "\n",
        "    # Confidence gap analysis\n",
        "    print(\"🔍 CONFIDENCE GAP ANALYSIS:\")\n",
        "    for eval_name, eval_results in results.items():\n",
        "        eval_display = eval_name.replace('_', ' ').title()\n",
        "        print(f\"\\n   {eval_display}:\")\n",
        "        print(f\"     ✅ Top-1 Hit Gap:     {eval_results['avg_gap_top1_hit']:.2f}%\")\n",
        "        print(f\"     ❌ Top-5 Hit Gap:    {eval_results['avg_error_top5_hit']:.2f}%\")\n",
        "        print(f\"     ❌ Top-10 Hit Gap:   {eval_results['avg_error_top10_hit']:.2f}%\")\n",
        "        print(f\"     ❌ Top-10 Miss Gap:  {eval_results['avg_error_top10_miss']:.2f}%\")\n",
        "\n",
        "def save_results(results, filename=\"clip_evaluation_results.json\"):\n",
        "    \"\"\"Save results to JSON file for later analysis\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aUau_-YT_XmU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zTnF4b47HMA",
        "outputId": "0b5ee181-83d3-410d-988d-dd8905e37da4",
        "cellView": "form",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENAI_KEY not found in Colab secrets. Please add it to proceed.\n",
            "Google GenAI SDK configured successfully!\n",
            "Pydantic schema defined.\n"
          ]
        }
      ],
      "source": [
        "#@title Gemini setup\n",
        "# Load your API key from Colab secrets.\n",
        "try:\n",
        "    GENAI_API_KEY = userdata.get('GENAI_KEY')\n",
        "    genai.configure(api_key=GENAI_API_KEY)\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"GENAI_KEY not found in Colab secrets. Please add it to proceed.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during API key setup: {e}\")\n",
        "    exit()\n",
        "\n",
        "print(\"Google GenAI SDK configured successfully!\")\n",
        "\n",
        "# Define the Pydantic schema for the LLM output\n",
        "class PromptDescriptions(BaseModel):\n",
        "    # This will hold a list of lists: [[desc1_flower1, desc2_flower1, ...], [desc1_flower2, ...], ...]\n",
        "    flower_descriptions: List[List[str]] = Field(\n",
        "        description=\"A list where each element is a list of 5 short descriptions for a specific flower.\"\n",
        "    )\n",
        "\n",
        "print(\"Pydantic schema defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqPWs3Wk8tM_",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 3. LLM Prompt Generation (Batched Calls)\n",
        "\n",
        "genai_model = genai.GenerativeModel('gemini-2.5-flash')  # Using Flash for speed\n",
        "\n",
        "generated_prompts_for_classes = {}  # {class_name: [list of 5 prompts]}\n",
        "prompt_batch_size = 13\n",
        "\n",
        "# Define schema using Pydantic\n",
        "class Prompts(BaseModel):\n",
        "    prompt1: str\n",
        "    prompt2: str\n",
        "    prompt3: str\n",
        "    prompt4: str\n",
        "    prompt5: str\n",
        "\n",
        "class PromptDescriptions(BaseModel):\n",
        "    flower_descriptions: List[Prompts]\n",
        "\n",
        "print(\"\\n--- Generating 5 prompts for each flower class using LLM ---\")\n",
        "print(\"This might take a few minutes depending on the number of classes and API response times.\")\n",
        "\n",
        "for i in tqdm(range(0, len(CLASS_NAMES), prompt_batch_size), desc=\"Generating prompts in batches\"):\n",
        "    current_batch_classes = CLASS_NAMES[i : min(i + prompt_batch_size, len(CLASS_NAMES))]\n",
        "\n",
        "    # Dynamically build the prompt for the current batch\n",
        "    prompt_batch = f\"\"\"You are a professional botanical photographer and creative writer for a visual nature magazine.\n",
        "\n",
        "Given a list of flower species, generate exactly 5 short and visually evocative descriptions per flower. Each description should be phrased as if accompanying a stunning photograph of the flower, aiming to evoke its unique beauty and essence for an AI model like CLIP.\n",
        "\n",
        "Each prompt must be:\n",
        "- Descriptive, with strong visual language (color, shape, textures, dimension of petals, feeling)\n",
        "- Distinct across the 5 prompts (no overlap or simple rewordings)\n",
        "- Specific to the given flower (no generic phrases)\n",
        "- Emphasize features that distinguish this flower from visually similar species\n",
        "- Realistic and natural (imagine you're writing the description of your own photo of the flower)\n",
        "\n",
        "Use the word 'flower' in at least 2 out of 5 prompts. Prioritize conciseness (max 20 words) and features that distinguish the flower.\n",
        "\n",
        "Return the output as a JSON object with a single key 'flower_descriptions', whose value is a list of objects. Each object corresponds to a flower and includes exactly 5 fields: 'prompt1' through 'prompt5' (all strings).\n",
        "\n",
        "Here are the flowers for this batch: {current_batch_classes} \"\"\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        response_llm = genai_model.generate_content(\n",
        "            prompt_batch,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                response_mime_type=\"application/json\",\n",
        "                response_schema=PromptDescriptions\n",
        "            )\n",
        "        )\n",
        "\n",
        "        parsed_response = PromptDescriptions.model_validate_json(response_llm.text)\n",
        "\n",
        "        for j, prompt_obj in enumerate(parsed_response.flower_descriptions):\n",
        "            flower_name = current_batch_classes[j]\n",
        "            generated_prompts_for_classes[flower_name] = [\n",
        "                prompt_obj.prompt1,\n",
        "                prompt_obj.prompt2,\n",
        "                prompt_obj.prompt3,\n",
        "                prompt_obj.prompt4,\n",
        "                prompt_obj.prompt5,\n",
        "            ]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError generating prompts for batch starting with '{current_batch_classes[0]}': {e}\")\n",
        "        print(f\"Raw response (if available): {response_llm.text if 'response_llm' in locals() else 'N/A'}\")\n",
        "\n",
        "print(\"\\nLLM prompt generation complete.\")\n",
        "\n",
        "with open(\"generated_prompts.json\", \"w\") as f:\n",
        "    json.dump(generated_prompts_for_classes, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Evaluation\n",
        "results = run_evaluation_suite()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "imR1Bvc8GS1i",
        "outputId": "3a101d6b-7ac5-421b-de6b-737761936cef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting memory-optimized CLIP evaluation suite...\n",
            "✅ Found generated prompts file\n",
            "📊 Loaded prompts for 102 classes\n",
            "🤖 Loading CLIP model: ViT-B/16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:05<00:00, 61.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CLIP model loaded on cuda\n",
            "📱 Using device: cuda\n",
            "\n",
            "📥 Loading and preparing datasets...\n",
            "📥 Downloading and loading Flowers102 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 345M/345M [00:13<00:00, 26.2MB/s]\n",
            "100%|██████████| 502/502 [00:00<00:00, 786kB/s]\n",
            "100%|██████████| 15.0k/15.0k [00:00<00:00, 24.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset loaded - Train: 1020, Val: 1020, Test: 6149\n",
            "📊 Class split - Base: 51, Novel: 51\n",
            "📊 Data split - Base samples: 510, Novel samples: 510\n",
            "📊 Data split - Base samples: 510, Novel samples: 510\n",
            "📊 Data split - Base samples: 2473, Novel samples: 3676\n",
            "📊 Dataset prepared:\n",
            "   Base classes: 51 Novel classes: 51\n",
            "   Test base samples: 2473 Test novel samples: 3676\n",
            "\n",
            "============================================================\n",
            "🔄 EVALUATION 1: Standard Prompts on Base Classes\n",
            "============================================================\n",
            "📝 Generated 51 standard prompts\n",
            "📄 Example prompt: 'a photo of a pink primrose, a type of flower.'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 39/39 [00:27<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Total samples evaluated: 2473\n",
            "\n",
            "✅ Top-1 Accuracy:      71.25%\n",
            "✅ Top-5 Accuracy:      90.90%\n",
            "✅ Top-10 Accuracy:     97.53%\n",
            "✅ Avg. Conf. Gap (Top-1 hit):      3.27%\n",
            "❌ Avg. Conf. Gap (Top-5 hit):     1.67%\n",
            "❌ Avg. Conf. Gap (Top-10 hit):    3.88%\n",
            "❌ Avg. Conf. Gap (Beyond top-10): 5.01%\n",
            "\n",
            "============================================================\n",
            "🔄 EVALUATION 2: Standard Prompts on Novel Classes\n",
            "============================================================\n",
            "📝 Generated 51 standard prompts\n",
            "📄 Example prompt: 'a photo of a wild pansy, a type of flower.'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🧠 Zero-shot evaluation on Novel Classes: 100%|██████████| 58/58 [00:39<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Total samples evaluated: 3676\n",
            "\n",
            "✅ Top-1 Accuracy:      78.26%\n",
            "✅ Top-5 Accuracy:      89.15%\n",
            "✅ Top-10 Accuracy:     92.79%\n",
            "✅ Avg. Conf. Gap (Top-1 hit):      3.68%\n",
            "❌ Avg. Conf. Gap (Top-5 hit):     1.37%\n",
            "❌ Avg. Conf. Gap (Top-10 hit):    3.45%\n",
            "❌ Avg. Conf. Gap (Beyond top-10): 5.70%\n",
            "\n",
            "============================================================\n",
            "🔄 EVALUATION 3: LLM-Enhanced Prompts on Base Classes\n",
            "============================================================\n",
            "🤖 Processing LLM-generated prompts for 51 classes...\n",
            "📝 Processing 5 prompts for 'pink primrose'\n",
            "📝 Processing 5 prompts for 'hard-leaved pocket orchid'\n",
            "📝 Processing 5 prompts for 'canterbury bells'\n",
            "📝 Processing 5 prompts for 'sweet pea'\n",
            "📝 Processing 5 prompts for 'english marigold'\n",
            "📝 Processing 5 prompts for 'tiger lily'\n",
            "📝 Processing 5 prompts for 'moon orchid'\n",
            "📝 Processing 5 prompts for 'bird of paradise'\n",
            "📝 Processing 5 prompts for 'monkshood'\n",
            "📝 Processing 5 prompts for 'globe thistle'\n",
            "📝 Processing 5 prompts for 'snapdragon'\n",
            "📝 Processing 5 prompts for 'colt's foot'\n",
            "📝 Processing 5 prompts for 'king protea'\n",
            "📝 Processing 5 prompts for 'spear thistle'\n",
            "📝 Processing 5 prompts for 'yellow iris'\n",
            "📝 Processing 5 prompts for 'globe-flower'\n",
            "📝 Processing 5 prompts for 'purple coneflower'\n",
            "📝 Processing 5 prompts for 'peruvian lily'\n",
            "📝 Processing 5 prompts for 'balloon flower'\n",
            "📝 Processing 5 prompts for 'giant white arum lily'\n",
            "📝 Processing 5 prompts for 'fire lily'\n",
            "📝 Processing 5 prompts for 'pincushion flower'\n",
            "📝 Processing 5 prompts for 'fritillary'\n",
            "📝 Processing 5 prompts for 'red ginger'\n",
            "📝 Processing 5 prompts for 'grape hyacinth'\n",
            "📝 Processing 5 prompts for 'corn poppy'\n",
            "📝 Processing 5 prompts for 'prince of wales feathers'\n",
            "📝 Processing 5 prompts for 'stemless gentian'\n",
            "📝 Processing 5 prompts for 'artichoke'\n",
            "📝 Processing 5 prompts for 'sweet william'\n",
            "📝 Processing 5 prompts for 'carnation'\n",
            "📝 Processing 5 prompts for 'garden phlox'\n",
            "📝 Processing 5 prompts for 'love in the mist'\n",
            "📝 Processing 5 prompts for 'mexican aster'\n",
            "📝 Processing 5 prompts for 'alpine sea holly'\n",
            "📝 Processing 5 prompts for 'ruby-lipped cattleya'\n",
            "📝 Processing 5 prompts for 'cape flower'\n",
            "📝 Processing 5 prompts for 'great masterwort'\n",
            "📝 Processing 5 prompts for 'siam tulip'\n",
            "📝 Processing 5 prompts for 'lenten rose'\n",
            "📝 Processing 5 prompts for 'barbeton daisy'\n",
            "📝 Processing 5 prompts for 'daffodil'\n",
            "📝 Processing 5 prompts for 'sword lily'\n",
            "📝 Processing 5 prompts for 'poinsettia'\n",
            "📝 Processing 5 prompts for 'bolero deep blue'\n",
            "📝 Processing 5 prompts for 'wallflower'\n",
            "📝 Processing 5 prompts for 'marigold'\n",
            "📝 Processing 5 prompts for 'buttercup'\n",
            "📝 Processing 5 prompts for 'oxeye daisy'\n",
            "📝 Processing 5 prompts for 'common dandelion'\n",
            "📝 Processing 5 prompts for 'petunia'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🌸 Zero-shot eval with LLM prompts on Base Classes: 100%|██████████| 39/39 [00:26<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Total samples evaluated: 2473\n",
            "\n",
            "✅ Top-1 Accuracy:      79.18%\n",
            "✅ Top-5 Accuracy:      94.99%\n",
            "✅ Top-10 Accuracy:     96.24%\n",
            "✅ Avg. Conf. Gap (Top-1 hit):      3.85%\n",
            "❌ Avg. Conf. Gap (Top-5 hit):     1.51%\n",
            "❌ Avg. Conf. Gap (Top-10 hit):    4.15%\n",
            "❌ Avg. Conf. Gap (Beyond top-10): 7.14%\n",
            "\n",
            "============================================================\n",
            "🔄 EVALUATION 4: LLM-Enhanced Prompts on Novel Classes\n",
            "============================================================\n",
            "🤖 Processing LLM-generated prompts for 51 classes...\n",
            "📝 Processing 5 prompts for 'wild pansy'\n",
            "📝 Processing 5 prompts for 'primula'\n",
            "📝 Processing 5 prompts for 'sunflower'\n",
            "📝 Processing 5 prompts for 'pelargonium'\n",
            "📝 Processing 5 prompts for 'bishop of llandaff'\n",
            "📝 Processing 5 prompts for 'gaura'\n",
            "📝 Processing 5 prompts for 'geranium'\n",
            "📝 Processing 5 prompts for 'orange dahlia'\n",
            "📝 Processing 5 prompts for 'pink-yellow dahlia?'\n",
            "📝 Processing 5 prompts for 'cautleya spicata'\n",
            "📝 Processing 5 prompts for 'japanese anemone'\n",
            "📝 Processing 5 prompts for 'black-eyed susan'\n",
            "📝 Processing 5 prompts for 'silverbush'\n",
            "📝 Processing 5 prompts for 'californian poppy'\n",
            "📝 Processing 5 prompts for 'osteospermum'\n",
            "📝 Processing 5 prompts for 'spring crocus'\n",
            "📝 Processing 5 prompts for 'bearded iris'\n",
            "📝 Processing 5 prompts for 'windflower'\n",
            "📝 Processing 5 prompts for 'tree poppy'\n",
            "📝 Processing 5 prompts for 'gazania'\n",
            "📝 Processing 5 prompts for 'azalea'\n",
            "📝 Processing 5 prompts for 'water lily'\n",
            "📝 Processing 5 prompts for 'rose'\n",
            "📝 Processing 5 prompts for 'thorn apple'\n",
            "📝 Processing 5 prompts for 'morning glory'\n",
            "📝 Processing 5 prompts for 'passion flower'\n",
            "📝 Processing 5 prompts for 'lotus'\n",
            "📝 Processing 5 prompts for 'toad lily'\n",
            "📝 Processing 5 prompts for 'anthurium'\n",
            "📝 Processing 5 prompts for 'frangipani'\n",
            "📝 Processing 5 prompts for 'clematis'\n",
            "📝 Processing 5 prompts for 'hibiscus'\n",
            "📝 Processing 5 prompts for 'columbine'\n",
            "📝 Processing 5 prompts for 'desert-rose'\n",
            "📝 Processing 5 prompts for 'tree mallow'\n",
            "📝 Processing 5 prompts for 'magnolia'\n",
            "📝 Processing 5 prompts for 'cyclamen'\n",
            "📝 Processing 5 prompts for 'watercress'\n",
            "📝 Processing 5 prompts for 'canna lily'\n",
            "📝 Processing 5 prompts for 'hippeastrum'\n",
            "📝 Processing 5 prompts for 'bee balm'\n",
            "📝 Processing 5 prompts for 'ball moss'\n",
            "📝 Processing 5 prompts for 'foxglove'\n",
            "📝 Processing 5 prompts for 'bougainvillea'\n",
            "📝 Processing 5 prompts for 'camellia'\n",
            "📝 Processing 5 prompts for 'mallow'\n",
            "📝 Processing 5 prompts for 'mexican petunia'\n",
            "📝 Processing 5 prompts for 'bromelia'\n",
            "📝 Processing 5 prompts for 'blanket flower'\n",
            "📝 Processing 5 prompts for 'trumpet creeper'\n",
            "📝 Processing 5 prompts for 'blackberry lily'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🌸 Zero-shot eval with LLM prompts on Novel Classes: 100%|██████████| 58/58 [00:40<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Total samples evaluated: 3676\n",
            "\n",
            "✅ Top-1 Accuracy:      80.66%\n",
            "✅ Top-5 Accuracy:      92.19%\n",
            "✅ Top-10 Accuracy:     93.25%\n",
            "✅ Avg. Conf. Gap (Top-1 hit):      4.32%\n",
            "❌ Avg. Conf. Gap (Top-5 hit):     1.93%\n",
            "❌ Avg. Conf. Gap (Top-10 hit):    4.39%\n",
            "❌ Avg. Conf. Gap (Beyond top-10): 13.61%\n",
            "🧹 Cleaning up CLIP model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show results\n",
        "analyze_results(results=results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "94W5dlyoGcc1",
        "outputId": "046d1922-bfbd-4cea-b872-34658619500d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "📊 COMPREHENSIVE RESULTS ANALYSIS\n",
            "============================================================\n",
            "🔤 STANDARD PROMPTS RESULTS:\n",
            "   📈 Harmonic Mean (Top-1): 74.59%\n",
            "   🎯 Base Classes Top-1:    71.25%\n",
            "   🆕 Novel Classes Top-1:   78.26%\n",
            "   📊 Base Classes Top-5:    90.90%\n",
            "   📊 Novel Classes Top-5:   89.15%\n",
            "\n",
            "🤖 LLM-ENHANCED PROMPTS RESULTS:\n",
            "   📈 Harmonic Mean (Top-1): 79.91%\n",
            "   🎯 Base Classes Top-1:    79.18%\n",
            "   🆕 Novel Classes Top-1:   80.66%\n",
            "   📊 Base Classes Top-5:    94.99%\n",
            "   📊 Novel Classes Top-5:   92.19%\n",
            "\n",
            "📈 IMPROVEMENT WITH LLM PROMPTS:\n",
            "   🎯 Base Classes:    +7.93 percentage points\n",
            "   🆕 Novel Classes:   +2.39 percentage points\n",
            "   📈 Harmonic Mean:   +5.32 percentage points\n",
            "\n",
            "🔍 CONFIDENCE GAP ANALYSIS:\n",
            "\n",
            "   Standard Base:\n",
            "     ✅ Top-1 Hit Gap:     3.27%\n",
            "     ❌ Top-5 Hit Gap:    1.67%\n",
            "     ❌ Top-10 Hit Gap:   3.88%\n",
            "     ❌ Top-10 Miss Gap:  5.01%\n",
            "\n",
            "   Standard Novel:\n",
            "     ✅ Top-1 Hit Gap:     3.68%\n",
            "     ❌ Top-5 Hit Gap:    1.37%\n",
            "     ❌ Top-10 Hit Gap:   3.45%\n",
            "     ❌ Top-10 Miss Gap:  5.70%\n",
            "\n",
            "   Llm Base:\n",
            "     ✅ Top-1 Hit Gap:     3.85%\n",
            "     ❌ Top-5 Hit Gap:    1.51%\n",
            "     ❌ Top-10 Hit Gap:   4.15%\n",
            "     ❌ Top-10 Miss Gap:  7.14%\n",
            "\n",
            "   Llm Novel:\n",
            "     ✅ Top-1 Hit Gap:     4.32%\n",
            "     ❌ Top-5 Hit Gap:    1.93%\n",
            "     ❌ Top-10 Hit Gap:   4.39%\n",
            "     ❌ Top-10 Miss Gap:  13.61%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}